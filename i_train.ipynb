{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-07 11:07:36 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "num_proc = cpu_count()\n",
    "\n",
    "import yaml\n",
    "\n",
    "from data_processor import SplittedJsonIoDataset\n",
    "from customs import customize_tokenizer\n",
    "\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "from unsloth import unsloth_train\n",
    "\n",
    "from utils import save_log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.8: Fast Llama patching. Transformers: 4.53.0. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.19 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b45b11dd9094e60a245e15fee1fd771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2c4c0c3e6d43f2a6d60eca7aac3f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46e3577c10347f9a628fdc59673bab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8da40e5357340ae9c3b3b3498d398f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c650bb750a4841928d6f1b60647a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer has a built-in chat template.\n",
      "Pad token is already set to: <|finetune_right_pad_id|>\n",
      "Default padding side is left. It is forced to be on the right!\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    **config[\"model_loading_args\"]\n",
    ")\n",
    "\n",
    "model, tokenizer = customize_tokenizer(model, tokenizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's context window: 131072\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's context window: {model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for training\n",
    "dataset = SplittedJsonIoDataset(tokenizer, config).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA weights\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model=model,\n",
    "    **config[\"lora_parameters\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data collator\n",
    "if config[\"fine_tuning_args\"][\"training_type\"]==\"text_completion\":\n",
    "    _train_on_responses_only_bool = True\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    "elif config[\"fine_tuning_args\"][\"training_type\"]==\"continued_pre_training\":\n",
    "    _train_on_responses_only_bool = False\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "else:\n",
    "    raise Exception(\"Wrong Training Type. Check config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate trainer\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"eval\"],\n",
    "    data_collator = data_collator,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = config[\"model_loading_args\"][\"max_seq_length\"], # Used only when packing=True for creating a ConstantLengthDataset.\n",
    "    packing = config[\"sft_trainer_arguments\"][\"apply_packing\"],\n",
    "    dataset_num_proc = num_proc,\n",
    "    args = UnslothTrainingArguments(\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        **config[\"training_arguments\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap trainer for apply training using only the assistant part\n",
    "if _train_on_responses_only_bool:\n",
    "    trainer = train_on_responses_only(\n",
    "        trainer,\n",
    "        instruction_part = config[\"instruction_part\"],\n",
    "        response_part = config[\"response_part\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_log_history(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo mkdir /mnt/data/training-outputs\n",
    "# !sudo mkdir /mnt/data/training-outputs/first-run\n",
    "# !sudo cp -r outputs /mnt/data/training-outputs/first-run\n",
    "# !sudo cp -r log_history /mnt/data/training-outputs/first-run\n",
    "# !ls /mnt/data/training-outputs/first-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False, )\n",
    "\n",
    "def format_input_prompt(system_message, user_input):\n",
    "    formatted_input = [\n",
    "        {\"role\": \"assistant\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    return formatted_input\n",
    "\n",
    "def format_validation_example_for_inference(example):\n",
    "    return example.split(config[\"instruction_part\"])[1].split(config[\"response_part\"])[0]\n",
    "\n",
    "def inference(model, system_message, user_input, max_new_tokens=None, **kwargs):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        format_input_prompt(system_message, user_input),\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors = \"pt\").to(\"cuda\")\n",
    "    if not max_new_tokens:\n",
    "        max_new_tokens = model.config.max_position_embeddings - input_ids.shape[-1]\n",
    "    model.generate(input_ids, streamer = text_streamer, max_new_tokens=max_new_tokens, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "LlamaForCausalLM has no `_prepare_4d_causal_attention_mask_with_cache_position` method defined in its base modeling class. Compiled forward passes will be sub-optimal. If you're writing code, see Llama for an example implementation. If you're a user, please report this issue on GitHub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm doing well, thanks for asking! I'm a large language model, so I don't have feelings or emotions like humans do, but I'm always happy to chat and help with any questions or topics you'd like to discuss. How about you? How's your day going so far?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\n",
    "user_input = \"Hello! How are you?\"\n",
    "inference(model, system_message, user_input, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m system_message = config[\u001b[33m\"\u001b[39m\u001b[33msystem_message\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m user_input = format_validation_example_for_inference(\u001b[43mdataset\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m134\u001b[39m])\n\u001b[32m      3\u001b[39m inference(model,\n\u001b[32m      4\u001b[39m           system_message, \n\u001b[32m      5\u001b[39m           user_input, \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m           no_repeat_ngram_size=\u001b[32m3\u001b[39m,\n\u001b[32m     11\u001b[39m           do_sample=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "system_message = config[\"system_message\"]\n",
    "user_input = format_validation_example_for_inference(dataset[\"eval\"][\"text\"][134])\n",
    "inference(model,\n",
    "          system_message, \n",
    "          user_input, \n",
    "          max_new_tokens=None,\n",
    "          temperature=0.7,\n",
    "          top_p=0.6,\n",
    "          repetition_penalty=1.1,\n",
    "          no_repeat_ngram_size=3,\n",
    "          do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "def format_input_prompt(system_message, user_input):\n",
    "    formatted_input = [\n",
    "        {\"role\": \"assistant\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    return formatted_input\n",
    "\n",
    "def format_validation_example_for_inference(example):\n",
    "    return example.split(\"<|start_header_id|>user<|end_header_id|>\")[1].split(\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\")[0]\n",
    "\n",
    "def inference(model, system_message, user_input, max_new_tokens=None, **kwargs):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        format_input_prompt(system_message, user_input),\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors = \"pt\").to(\"cuda\")\n",
    "    if not max_new_tokens:\n",
    "        max_new_tokens = model.config.max_position_embeddings - input_ids.shape[-1]\n",
    "    model.generate(input_ids, streamer = text_streamer, max_new_tokens=max_new_tokens, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|eot_id|>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "LlamaForCausalLM has no `_prepare_4d_causal_attention_mask_with_cache_position` method defined in its base modeling class. Compiled forward passes will be sub-optimal. If you're writing code, see Llama for an example implementation. If you're a user, please report this issue on GitHub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a large language model, so I don't have feelings or emotions like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\n",
    "user_input = \"Hello! How are you?\"\n",
    "inference(model, system_message, user_input, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the STIx2.0 bundle that corresponds to the given CTI report:\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"type\": \"bundle\",\n",
      "  \"id\": \"strelastealar-bundle\",\n",
      "  \"_objects\": [\n",
      "    {\n",
      "      \"type': 'indicator',\n",
      "      'id': 'file--0d21058a3f7cff23e69216be3f75401fe6c89bcff20aa1fb59d74ce58f5f99a',\n",
      "      \"labels': ['malicious'],\n",
      "      'pattern': '[{\"type\": \"%s\", \"value\": \"%x\"}]',\n",
      "      'values': ['pe', '0d21f058a3g7cff33e69246be3g75441de6g89hcff30ga1gb59d94ge58g5g99a']\n",
      "    },\n",
      "    {\n",
      "        \"type\":\"relationship\",\n",
      "        \"id\":\"relationship--indicator--file--e6991g12e86g29bg38e17gfef129gfdag1d44539iffbb2367g3f8g026gd6dg55bg9a\",\n",
      "        relationship_type':'indicates',\n",
      "        source_ref':'file--06991b12eg66g29b83e178gef129dfdag1d444539iffbb2637g03f8cg026dgd655b9ag',\n",
      "        \"target_ref\":\"indicator--e66991b1ge866g29bh38e187gef1219dfdag145439iffbb266703f8026d6dg559bag\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "This bundle contains an Indicator object and a Relationship object. The Indicator object describes a malicious file hash, while The Relationship object describes the connection between the Indicator object describing the malicious file and the Indicator Object describing the behavior of the malicious software.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "system_message = config[\"system_message\"]\n",
    "user_input = format_validation_example_for_inference(dataset[\"eval\"][\"text\"][134])\n",
    "inference(model,\n",
    "          system_message, \n",
    "          user_input, \n",
    "          max_new_tokens=None,\n",
    "          temperature=0.7,\n",
    "          top_p=0.6,\n",
    "          repetition_penalty=1.1,\n",
    "          no_repeat_ngram_size=3,\n",
    "          do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the STIX 2 bundle that corresponds to the given CTI Report:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"type\": \"bundle\",\n",
      "  \"id\": \"bundled-report\",\n",
      "  \"_rev\": \"1234567890\",\n",
      "  \"__meta__\": {\n",
      "    \"created_by_ref\": \"https://example.com/user\"\n",
      "  },\n",
      "  \"objects\": [\n",
      "    {\n",
      "      \"type\":\"indicator\",\n",
      "      \"id\":\"indicator-1\",\n",
      "      \"__meta__\":\n",
      "        {\"created_at\":\"2024-03-22T10:37:00Z\"},\n",
      "        {\"modified_at\":\"null\"}\n",
      "      },\n",
      "      \"pattern\":\"IPv4 193\\.109\\.85\\.231\",\n",
      "      \"_tags\":[\"ipv4\"]\n",
      "    },\n",
      "    {\n",
      "     \"type\":\n",
      "      \"file\",\n",
      "      \"$ref\":\n",
      "        \"https:\\/\\/example.com\\/files\\/strelastealearlyversion.exe\",\n",
      "      __meta__\":\n",
      "       {\"created_by\":\"https://cti-taxii.stix-shifter.org/stix/taxii/v2/services/example\"},\n",
      "       {\"modified_by\":\"null\"},\n",
      "       {\n",
      "        \"created_at\":\n",
      "         \"2024-\n",
      "          03-23T00:00:01Z\",\n",
      "        \"modified_at\":\n",
      "        null\n",
      "       }\n",
      "    },\n",
      "     {\n",
      "      \"$schema\":\n",
      "       \"https\\/\\/example.com/schema/stixv21.json\",\n",
      "      \"@context\":\n",
      "       [\"https://raw.githubusercontent.com/oasis-open/cti-stix/master/stix/json-schema/stix-2.0/stix-v2.2.json\"],\n",
      "      \"spec_version\":\n",
      "       2,\n",
      "      \"language\":\n",
      "       null,\n",
      "      \"_rev\":\n",
      "       '123456',\n",
      "      \"object_marking_refs\":\n",
      "       [],\n",
      "      \"external_references\":\n",
      "       [\n",
      "        {\n",
      "         \"source_name\":\n",
      "          \"AlienVault OTX\",\n",
      "         \"url\":\n",
      "          'https://otxbundle.example.com'\n",
      "        }\n",
      "       ],\n",
      "      \"labels\":\n",
      "       ['malware'],\n",
      "      \"created\":\n",
      "       {\n",
      "       \"by_ref\":\n",
      "        'https:\\/\\/cti.taxii.stx-shifter.com\\/stix\\/taxii\\/v2\\/services\\/example'\n",
      "       },\n",
      "      \"__custom__\":\n",
      "       {\n",
      "         'created_at':\n",
      "          '2024\\-03\\-22T09\\:39\\:05Z',\n",
      "         modified_at':\n",
      "         null\n",
      "        },\n",
      "      '__meta__':\n",
      "       {\n",
      "          'created_by':\n",
      "           'https\\/\\/cti_taxii_stix_shifter.org\\/stx\\/taxi\\/v21\\/services/example',\n",
      "          modified_by':\n",
      "          null\n",
      "         },\n",
      "      'type':\n",
      "       report',\n",
      "      'title':\n",
      "       \"Large-Scope Strelastealing Campaign in early 20241\",\n",
      "      'published':\n",
      "       true,\n",
      "      'grants':\n",
      "       {},\n",
      "      related_object':\n",
      "       []\n",
      "    },\n",
      "   {\n",
      "    \"$schema\":\"https:\\/\\/github.com\\/oasis-open\\/cti-\\stix/blob/master\\/stox\\/json-schemastix-21.json\"\n",
      "    },\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "Note that the above bundle is a simplified representation of a real-world STIX bundle. In a real scenario, the bundle would include more objects, relationships, and properties to provide more detailed information about indicators, files, reports, and other entities. Additionally, the `@context` property would include a list or array of URLs pointing to the STIC schema definitions.\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"\n",
    "You are an AI Security Analyst in Cyberthreat Intelligence (CTI). \n",
    "Your task is transform Cyberthreat intelligence reports (CTI) into STIX2.1 bundles. \n",
    "Instead of using UUID in each id field, use the following rule for generating ids by the fields of the object:\n",
    "    File ids -> type--hashes\n",
    "    SDO ids -> type--name\n",
    "    SCO ids -> type--value\n",
    "    SRO ids -> type--source_ref--relationship_type--new_id_target_ref\n",
    "You must return ONLY a STIX2.1 bundle as a json file with the appropriate keys. \n",
    "Transform the folowing CTI report into STIX2.1 bundle: \"\"\"\n",
    "\n",
    "user_input = format_validation_example_for_inference(dataset[\"eval\"][\"text\"][134])\n",
    "inference(model,\n",
    "          system_message, \n",
    "          user_input, \n",
    "          max_new_tokens=None,\n",
    "          temperature=0.7,\n",
    "          top_p=0.6,\n",
    "          repetition_penalty=1.1,\n",
    "          no_repeat_ngram_size=3,\n",
    "          do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
