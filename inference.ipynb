{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6fa3810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 09-03 11:44:57 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizerBase\n",
    "\n",
    "from typing import Optional, Any\n",
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "\n",
    "\n",
    "class TransformersLLM(CustomLLM):\n",
    "\n",
    "    model_name: str\n",
    "    fast_inference: bool\n",
    "    load_in_4bit: bool\n",
    "    max_seq_length: Optional[int] = None\n",
    "    gpu_memory_utilization: Optional[float] = 0.8\n",
    "    context_window: Optional[int] = None\n",
    "    num_output: Optional[int] = None\n",
    "    model:PreTrainedModel = None\n",
    "    tokenizer:PreTrainedTokenizerBase = None\n",
    "\n",
    "    def __init__(self, **kwargs: Any):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = self.model_name,\n",
    "            fast_inference = self.fast_inference,\n",
    "            load_in_4bit = self.load_in_4bit,\n",
    "            max_seq_length = self.max_seq_length,\n",
    "            gpu_memory_utilization = self.gpu_memory_utilization\n",
    "        )\n",
    "\n",
    "        self.model = FastLanguageModel.for_inference(model)\n",
    "        self.tokenizer = tokenizer\n",
    "        if not self.context_window:\n",
    "            self.context_window = self.model.config.max_position_embeddings\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            model_name=self.model_name,\n",
    "            load_in_4bit=self.load_in_4bit,\n",
    "            max_seq_length=self.max_seq_length,\n",
    "            fast_inference=self.fast_inference,\n",
    "            gpu_memory_utilization=self.gpu_memory_utilization,\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output\n",
    "        )\n",
    "    \n",
    "    def format_input_prompt(self, system_message, user_input):\n",
    "        formatted_input = [\n",
    "            {\"role\": \"assistant\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        return formatted_input\n",
    "\n",
    "    def format_response(self, response):\n",
    "        return response.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1].replace(\"<|eot_id|>\", \"\")\n",
    "    \n",
    "    def inference(self, system_message, user_input, max_new_tokens=None, **kwargs):\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            self.format_input_prompt(system_message, user_input),\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors = \"pt\").to(\"cuda\")\n",
    "        if not max_new_tokens:\n",
    "            max_new_tokens = self.model.config.max_position_embeddings - input_ids.shape[-1]\n",
    "\n",
    "        output_ids = self.model.generate(input_ids, max_new_tokens=max_new_tokens, **kwargs)\n",
    "        response = self.tokenizer.batch_decode(output_ids)\n",
    "        actual_response = self.format_response(response[0])\n",
    "        return actual_response\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt:str, system_message: Optional[str], user_input:Optional[str], **kwargs: Any) -> CompletionResponse:\n",
    "        response = self.inference(system_message,\n",
    "                                  user_input,\n",
    "                                  max_new_tokens=self.num_output,\n",
    "                                  **kwargs)\n",
    "        return CompletionResponse(text=response)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt:str, system_message: Optional[str], user_input:Optional[str], **kwargs: Any) -> CompletionResponseGen:\n",
    "        response = self.inference(system_message,\n",
    "                                  user_input,\n",
    "                                  max_new_tokens=self.num_output,\n",
    "                                  **kwargs)\n",
    "        for token in self.dummy_response:\n",
    "            response += token\n",
    "            yield CompletionResponse(text=response, delta=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2312a913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.8: Fast Llama patching. Transformers: 4.53.0. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.19 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1f22637bf040adbfc1ef771b6f63a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = TransformersLLM(\n",
    "            model_name=\"/mnt/data/training-outputs/cti-model\",\n",
    "            fast_inference=False,\n",
    "            load_in_4bit=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5ddccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "system_message = \"\"\"You are an AI Security Analyst in Cyberthreat Intelligence (CTI).\n",
    "    Your task is to identify all domain names referenced in a CTI report.\n",
    "    You MUST return a json with a field \"objects\" being a list of json objects\n",
    "    that describe domain names.\n",
    "    To describe a domain name you should provide the fields id, type and value.\n",
    "    Instead of using UUID in the id field, use the rule type--value for generating ids.\n",
    "    If no domain names are identified return a json with an empty list \"objects\".\n",
    "    Identify all domain names in the following CTI report:\"\"\"\n",
    "\n",
    "with open(\"/mnt/data/openCTI/io-pairs/test/000e110f-3b22-46e0-b7db-9f121d818236.json\") as f:\n",
    "    user_input = json.load(f)[\"input\"]\n",
    "\n",
    "resp = llm.complete(prompt=\"\", \n",
    "                    system_message=system_message, \n",
    "                    user_input=user_input,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.6,\n",
    "                    repetition_penalty=1.1,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e86611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"objects\": [{\"id\": \"domain-name--bolt-food.site\", \"type\": \"url\", \"value\": \"bolt-food(site)\"}, {\"id\": (\"domain-name-\" + \"boltfood.site\"), \"type\":\"url\",\"value\":\"boltfood(site)\" }]}\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
