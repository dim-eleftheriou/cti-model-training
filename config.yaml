#############################
# Data args & configuration #
#############################
system_message: |
  You are an AI Security Analyst in Cyberthreat Intelligence (CTI). Your task is to:
  (1) identify any of the following STIX Domain Objects (SDOs), STIX Cyber-observable Objects (SCOs)
  and STIX Relationship Objects (SROs) in a given report and
  (2) generate the respective STIX2.1 bundle that represents your findings.
  It follows the list of SDOs, SCOs and SROs that you are searching for as well as the mandatory and optional details you should
  include in the bundle for each one.

  ### SROs ###
  relationship -> id, type , relationship_type, source_ref, target_ref, description (Optional)

  ### SCOs ###
  domain-name -> id, type, value
  hostname -> id, type, value
  url -> id, type, value
  email-addr -> id, type, value
  ipv4-addr -> id, type, value
  cryptocurrency-wallet -> id, type, value

  ### SDOs ###
  indicator -> id, type, name, description (Optional), indicator_types (Optional list), pattern, pattern_type, pattern_type (Should one of "stix", "snort", "yara")
  file -> id, type, name (Optional), hashes (Should be json), size (Optional), mime_type (Optional)
  attack-pattern -> id, type, name, description (Optional), aliases (Optional)
  identity -> id, type, name, description (Optional)
  malware -> id, type, name, description (Optional), malware_types (Optional), is_family (Optional), aliases (Optional), os_execution_envs (Optional), architecture_execution_envs (Optional), implementation_languages (Optional)
  report -> id, type, name, description (Optional), labels:list, report_types (Optional list), created (Should be datetime), object_refs (Should be a list)
  location -> id, type, name, description (Optional), country, description (Optional), latitude (Optional), longtitude (Optional), city (Optional)
  vulnerability -> id, type, name, description (Optional)
  intrusion-set -> id, type, name, description (Optional), aliases (Optional), goals (Optional), resource_level (Optional), primary_motivation (Optional), secondary_motivation (Optional)

  Instead of using UUID in each id field, use the following rule for generating ids by the fields of the object:
  File ids -> type--hashes
  SDO ids -> type--name
  SCO ids -> type--value
  SRO ids -> type--source_ref--relationship_type--new_id_target_ref

  Return ONLY the STIX2.1 bundle.

chat_template: #"llama-3.2"
instruction_part: <｜User｜> #"<|start_header_id|>user<|end_header_id|>"
response_part: <｜Assistant｜> #"<|start_header_id|>assistant<|end_header_id|>"
# Filter dataset by number of tokens. It will be equal to max_seq_length if max_seq_length not None
filter_dataset: False
# Dataset Paths
io_dataset_path: /mnt/data/openCTI/io-pairs

###################################
# Model selection & configuration #
###################################
model_loading_args:

  # If None the default will be used! Model's max_position_embeddings argument in config file. Default for Llama3.1 is 131072
  max_seq_length: #65536
  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
  dtype: 
  # Use 4bit quantization to reduce memory usage.
  load_in_4bit: True
  # Use 8bit quantization to reduce memory usage.
  load_in_8bit: False
  # Do not quantize the model. This is used for LoRA
  full_finetuning: False
  # Select model name from huggingface
  model_name: meta-llama/Llama-3.3-70B-Instruct #deepseek-ai/DeepSeek-R1-0528-Qwen3-8B #unsloth/Llama-3.2-1B

model_saving_args:

  # Select if trained model should be uploaded to hugging face
  push_to_hub: False
  # Select if trained model should be uploaded to hugging face in private mode
  private: True
  # Select model name to used for uploading to huggingface after training
  adapters_name: CTI-Lora-Model
  # Select if trained model should be saved as GGUF
  save_to_gguf: False
  # Select if trained model should be uploaded to hugging face in GGUF format
  push_to_hub_gguf: False
  # Quantization methods for GGUF saving
  quantization_method_gguf: ["not_quantized", "f16", "q8_0"]
  # Online directory for pushing model to hugging face
  online_directory: dim-eleftheriou
  # Online name
  online_name: DarkWatch

##################################
# Fine-tuning mode configuration #
##################################
fine_tuning_args:

  # Select the type of training to be performed. Must be either text_completion or continued_pre_training
  training_type: text_completion
  # Select if validation data should be used in training process
  use_validation_dataset: True
  # Add special tokens in training. It can increase too much the GPU consumption!
  special_tokens_list: []

###################
# LoRA parameters #
###################
lora_parameters:

  r: 8
  lora_alpha: 8
  lora_dropout: 0    # Supports any, but = 0 is optimized
  bias: none    # Supports any, but = "none" is optimized
  use_gradient_checkpointing: unsloth # True or "unsloth" for very long context
  use_rslora: True
  loftq_config: None
  # If continued_pre_training=True then all modules will be targeted.
  # Available modules are: "q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lm_head", "embed_tokens"
  target_modules: 
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - lm_head

########################
# SFTTrainer arguments #
########################
sft_trainer_arguments:

  data_collator: DataCollatorForSeq2Seq
  # Select number of processes to use for processing the dataset.
  dataset_num_proc: 4
  # Apply packing to dataset for fixed length. Creates a ConstantLengthDataset. Can decrease significantly the training time.
  apply_packing: False # To be used only for continued pre-training, otherwise may affect a lot the accuracy of the model.

######################
# Training Arguments #
######################
training_arguments:

  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  report_to: tensorboard
  warmup_ratio: 0.1
  num_train_epochs: 2
  learning_rate: 0.00005
  embedding_learning_rate: 0.000005
  logging_steps: 1
  optim: "adamw_8bit"
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  seed: 1234

  ########################
  # Evaluation Arguments #
  ########################
  fp16_full_eval: True
  per_device_eval_batch_size: 1
  eval_accumulation_steps: 16
  #eval_steps: 1
  eval_strategy: "steps"

  ####################
  # Saving arguments #
  ####################
  #logging_dir: "logs"
  logging_strategy: "steps"
  output_dir: "deepseek-qwen3-sft-outputs"
  save_strategy: "best"
  metric_for_best_model: "loss"
  #load_best_model_at_end: True
  save_total_limit: 1