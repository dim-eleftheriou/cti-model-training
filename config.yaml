#############################
# Data args & configuration #
#############################
system_message: "You are a helpful AI assistant that transforms Cyberthreat intelligence reports (CTI) into STIX2.1 bundles. You must return ONLY a STIX2.1 bundle as a json file with the appropriate keys. Transform the folowing CTI report into STIX2.1 bundle: "
chat_template: "llama-3.2"
instruction_part: "<|start_header_id|>user<|end_header_id|>"
response_part: "<|start_header_id|>assistant<|end_header_id|>"

###################################
# Model selection & configuration #
###################################
model_loading_args:

  # If None the default will be used! Accessible by tokenizer.model_max_length
  max_seq_length: 32768
  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
  dtype: 
  # Use 4bit quantization to reduce memory usage. Can be False.
  load_in_4bit: True
  # Select model name from huggingface
  model_name: meta-llama/Llama-3.2-1B #unsloth/Llama-3.2-1B

model_uploading_args:

  # Select if trained model should be uploaded to hugging face
  push_to_hub: True
  # Select if trained model should be uploaded to hugging face in private mode
  private: False
  # Select model name to used for uploading to huggingface after training
  model_name_for_saving: Llama-3.1-8B-Instruct
  # Quantization method for storing the model merged with adapters. Could more than one!
  quantization_method: ["q4_k_m", "q8_0"]

##################################
# Fine-tuning mode configuration #
##################################
fine_tuning_args:

  # Select the type of training to be performed. Must be either text_completion or continued_pre_training
  training_type: text_completion
  # Select if validation data should be used in training process
  use_validation_dataset: True
  # Add special tokens in training. It can increase too much the GPU consumption!
  special_tokens_list: []
  # Apply packing to dataset for fixed length. Creates a ConstantLengthDataset
  apply_packing: False

###################
# LoRA parameters #
###################
lora_parameters:

  r: 8
  lora_alpha: 8
  lora_dropout: 0    # Supports any, but = 0 is optimized
  bias: none    # Supports any, but = "none" is optimized
  use_gradient_checkpointing: unsloth # True or "unsloth" for very long context
  use_rslora: True
  loftq_config: None
  # If continued_pre_training=True then all modules will be targeted.
  # Available modules are: "q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lm_head", "embed_tokens"
  target_modules: 
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - lm_head

########################
# SFTTrainer arguments #
########################
sft_trainer_arguments:

  data_collator: DataCollatorForSeq2Seq
  # Select number of processes to use for processing the dataset.
  dataset_num_proc: 1

######################
# Training Arguments #
######################
training_arguments:

  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  report_to: none
  warmup_ratio: 0.1
  num_train_epochs: 3
  learning_rate: 0.00005
  embedding_learning_rate: 0.000005
  logging_steps: 1
  optim: "adamw_8bit"
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  seed: 1234

  ########################
  # Evaluation Arguments #
  ########################
  fp16_full_eval: True
  per_device_eval_batch_size: 1
  eval_accumulation_steps: 16
  #eval_steps: 1
  eval_strategy: "steps"

  ####################
  # Saving arguments #
  ####################
  output_dir: "outputs"
  save_strategy: "best"
  metric_for_best_model: "loss"
  #load_best_model_at_end: True
  save_total_limit: 1