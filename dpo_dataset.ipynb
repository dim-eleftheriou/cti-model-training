{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee1c15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 10-07 08:25:09 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from unsloth import FastLanguageModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d072b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = \"/mnt/data/training-outputs/Llama-3.1-8B-Malware-Expert/checkpoint-271\"\n",
    "\n",
    "sft_system_message = \"\"\"You are an AI Security Analyst in Cyberthreat Intelligence (CTI). \n",
    "                    Your task is to identify all malwares referenced or implied in a CTI report. \n",
    "                    You MUST return a json with a field \"objects\" being a list of json objects \n",
    "                    that describe malwares.\n",
    "                    To describe a malware you should provide the fields id, type, name and is_family.\n",
    "                    Instead of using UUID in the id field, use the rule type--name for generating ids.\n",
    "                    If no malwares are identified return a json with an empty list \"objects\".\n",
    "                    Identify all malwares in the folowing CTI report: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d631007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.8: Fast Llama patching. Transformers: 4.53.0. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb4ece3a6de41f09777dd5d76969633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = sft_model,\n",
    "    fast_inference = False,\n",
    "    load_in_4bit = False,\n",
    "    max_seq_length = None,\n",
    "    gpu_memory_utilization = 0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51e60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "def format_input_prompt(system_message, user_input):\n",
    "    formatted_input = [\n",
    "        {\"role\": \"assistant\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    return formatted_input\n",
    "\n",
    "def format_validation_example_for_inference(example):\n",
    "    return example.split(\"<|start_header_id|>user<|end_header_id|>\")[1].split(\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\")[0]\n",
    "\n",
    "def inference(model, system_message, user_input, max_new_tokens=None, **kwargs):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        format_input_prompt(system_message, user_input),\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors = \"pt\").to(\"cuda\")\n",
    "    if not max_new_tokens:\n",
    "        max_new_tokens = model.config.max_position_embeddings - input_ids.shape[-1]\n",
    "    model.generate(input_ids, streamer = text_streamer, max_new_tokens=max_new_tokens, **kwargs)\n",
    "\n",
    "def predict(model, system_message, user_input, max_new_tokens=None, **kwargs):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        format_input_prompt(system_message, user_input),\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors = \"pt\").to(\"cuda\")\n",
    "    if not max_new_tokens:\n",
    "        max_new_tokens = model.config.max_position_embeddings - input_ids.shape[-1]\n",
    "    \n",
    "    output_ids = model.generate(input_ids, max_new_tokens=max_new_tokens, **kwargs)\n",
    "    result = tokenizer.batch_decode(output_ids)\n",
    "    processed_result = result[0].split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1].split(\"<|eot_id|>\")[0]\n",
    "    return processed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3942d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path:str, filename:str):\n",
    "    with open(os.path.join(path, filename), mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def format_example(example:dict, system_message):\n",
    "        formatted_example = [\n",
    "            {\"role\": \"assistant\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps(example[\"output\"])}\n",
    "        ]\n",
    "        return formatted_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3686a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/mnt/data/openCTI/splitted-io-pairs/train\"\n",
    "eval_path = \"/mnt/data/openCTI/splitted-io-pairs/validation\"\n",
    "train_inputs = []\n",
    "train_outputs = []\n",
    "eval_inputs = []\n",
    "eval_outputs = []\n",
    "include_cti_type = [\"malware\"]\n",
    "\n",
    "for file in os.listdir(train_path):\n",
    "    cti_type = file.split(\"--\")[0]\n",
    "    if cti_type not in include_cti_type:\n",
    "        continue\n",
    "    example = load_json(train_path, file)\n",
    "    train_inputs.append(example[\"input\"])\n",
    "    train_outputs.append(example[\"output\"])\n",
    "\n",
    "for file in os.listdir(eval_path):\n",
    "    cti_type = file.split(\"--\")[0]\n",
    "    if cti_type not in include_cti_type:\n",
    "        continue\n",
    "    example = load_json(eval_path, file)\n",
    "    eval_inputs.append(example[\"input\"])\n",
    "    eval_outputs.append(example[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812a153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1858 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "LlamaForCausalLM has no `_prepare_4d_causal_attention_mask_with_cache_position` method defined in its base modeling class. Compiled forward passes will be sub-optimal. If you're writing code, see Llama for an example implementation. If you're a user, please report this issue on GitHub.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1858/1858 [1:19:43<00:00,  2.57s/it]\n"
     ]
    }
   ],
   "source": [
    "system_message = sft_system_message\n",
    "\n",
    "train_preds = [predict(model,\n",
    "                 system_message,\n",
    "                 user_input,\n",
    "                 max_new_tokens=500,\n",
    "                 temperature=0.6,\n",
    "                 top_p=0.6,\n",
    "                 repetition_penalty=1.1,\n",
    "                 no_repeat_ngram_size=3,\n",
    "                 do_sample=True) for user_input in tqdm(train_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78b1ddc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 301/301 [12:39<00:00,  2.52s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_preds = [predict(model,\n",
    "                 system_message,\n",
    "                 user_input,\n",
    "                 max_new_tokens=500,\n",
    "                 temperature=0.6,\n",
    "                 top_p=0.6,\n",
    "                 repetition_penalty=1.1,\n",
    "                 no_repeat_ngram_size=3,\n",
    "                 do_sample=True) for user_input in tqdm(eval_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58f85107",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        format_input_prompt(system_message, user_input),\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    ) for user_input in train_inputs\n",
    "]\n",
    "\n",
    "train_chosen = [json.dumps(p) + \"<|eot_id|>\" for p in train_outputs]\n",
    "train_rejected = [p + \"<|eot_id|>\" for p in train_preds]\n",
    "\n",
    "train_df = pd.DataFrame(\n",
    "    {\n",
    "        \"prompt\":train_prompts,\n",
    "        \"chosen\":train_chosen,\n",
    "        \"rejected\":train_rejected\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10ef0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.chosen!=train_df.rejected].to_csv(\"malware_train_data_for_DPO.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5b4b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        format_input_prompt(system_message, user_input),\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    ) for user_input in eval_inputs\n",
    "]\n",
    "\n",
    "eval_chosen = [json.dumps(p) + \"<|eot_id|>\" for p in eval_outputs]\n",
    "eval_rejected = [p + \"<|eot_id|>\" for p in eval_preds]\n",
    "\n",
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"prompt\":eval_prompts,\n",
    "        \"chosen\":eval_chosen,\n",
    "        \"rejected\":eval_rejected\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5279277",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df.chosen!=eval_df.rejected].to_csv(\"malware_eval_data_for_DPO.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b91dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
