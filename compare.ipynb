{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9d074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 10-20 09:56:55 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from unsloth import FastLanguageModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f77dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = \"/mnt/data/training-outputs/Llama-3.1-8B-Malware-Expert/checkpoint-271\"\n",
    "#sft_model = \"/mnt/data/training-outputs/Llama-3.1-8B-Malware-Expert-r128-a256/checkpoint-306\"\n",
    "\n",
    "#sft_model = \"/home/deleftheriou/cti-model-training/Llama-3.1-8B-Instruct-DPO-Malware-Expert/checkpoint-393\"\n",
    "\n",
    "sft_system_message = \"\"\"You are an AI Security Analyst in Cyberthreat Intelligence (CTI). \n",
    "                    Your task is to identify all malwares referenced or implied in a CTI report. \n",
    "                    You MUST return a json with a field \"objects\" being a list of json objects \n",
    "                    that describe malwares.\n",
    "                    To describe a malware you should provide the fields id, type, name and is_family.\n",
    "                    Instead of using UUID in the id field, use the rule type--name for generating ids.\n",
    "                    If no malwares are identified return a json with an empty list \"objects\".\n",
    "                    Identify all malwares in the folowing CTI report: \"\"\"\n",
    "\n",
    "# sft_system_message_2 = \"\"\"You are an AI Security Analyst in Cyberthreat Intelligence (CTI). \n",
    "#                  Your task is to identify all malwares referenced or implied in a CTI report. \n",
    "#                  You MUST return a json with a field \"objects\" being a list of json objects that describe malwares.\n",
    "#                  To describe a malware you should provide the fields id, type, name and is_family.\n",
    "#                  Instead of using UUID in the id field, use the rule type--name for generating ids.\n",
    "#                  For example, an output in which the malware RandomMalware is identified and is not family\n",
    "#                  of some other malware should be like this:\n",
    "                 \n",
    "#                  {\n",
    "#                      \"objects\": [\n",
    "#                          {\n",
    "#                              \"id\": \"malware--RandomMalware\",\n",
    "#                              \"type\": \"malware\",\n",
    "#                              \"name\": \"RandomMalware\",\n",
    "#                              \"is_family\": false\n",
    "#                          }\n",
    "#                      ]\n",
    "#                  }\n",
    "                 \n",
    "#                  If no malwares are identified return a json with an empty list \"objects\".\n",
    "#                  Identify all malwares in the folowing CTI report: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "899d9ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.8: Fast Llama patching. Transformers: 4.53.0. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.179 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a896bd28cc4540f8ae889c5990dfd6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436623a7fea5469f8a0666ec8fa182d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e86ec4f2bb43b198661c992ed53f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a228064d51f7481fa841eb341d2c6241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7347cf93e0f04679b5b5d13e62e28eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0aea2f9409041acb7a1f8a7966bc253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ee125e0bb043ce8fcb8897c9ad75cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1008.00 MiB. GPU 0 has a total capacity of 79.18 GiB of which 70.69 MiB is free. Process 51133 has 65.28 GiB memory in use. Including non-PyTorch memory, this process has 13.82 GiB memory in use. Of the allocated memory 13.22 GiB is allocated by PyTorch, and 15.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msft_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.8\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/unsloth/models/loader.py:434\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_peft:\n\u001b[32m    432\u001b[39m     \u001b[38;5;66;03m# From https://github.com/huggingface/peft/issues/184\u001b[39;00m\n\u001b[32m    433\u001b[39m     \u001b[38;5;66;03m# Now add PEFT adapters\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     model = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mold_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# Patch it as well!\u001b[39;00m\n\u001b[32m    443\u001b[39m     model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/peft/peft_model.py:541\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    533\u001b[39m     model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](\n\u001b[32m    534\u001b[39m         model,\n\u001b[32m    535\u001b[39m         config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    538\u001b[39m         low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    539\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m load_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n\u001b[32m    552\u001b[39m missing_keys = [\n\u001b[32m    553\u001b[39m     k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m load_result.missing_keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvblora_vector_bank\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprompt_encoder\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k\n\u001b[32m    554\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/peft/peft_model.py:1272\u001b[39m, in \u001b[36mPeftModel.load_adapter\u001b[39m\u001b[34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[39m\n\u001b[32m   1269\u001b[39m     peft_config.inference_mode = \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_adapter(adapter_name, peft_config, low_cpu_mem_usage=low_cpu_mem_usage)\n\u001b[32m-> \u001b[39m\u001b[32m1272\u001b[39m adapters_weights = \u001b[43mload_peft_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1274\u001b[39m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[32m   1275\u001b[39m ignore_mismatched_sizes = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mignore_mismatched_sizes\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:567\u001b[39m, in \u001b[36mload_peft_weights\u001b[39m\u001b[34m(model_id, device, **hf_hub_download_kwargs)\u001b[39m\n\u001b[32m    565\u001b[39m         adapters_weights = safe_load_file(filename, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m         adapters_weights = \u001b[43msafe_load_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    569\u001b[39m     adapters_weights = torch_load(filename, map_location=torch.device(device))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/safetensors/torch.py:315\u001b[39m, in \u001b[36mload_file\u001b[39m\u001b[34m(filename, device)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m safe_open(filename, framework=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, device=device) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f.keys():\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m         result[k] = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1008.00 MiB. GPU 0 has a total capacity of 79.18 GiB of which 70.69 MiB is free. Process 51133 has 65.28 GiB memory in use. Including non-PyTorch memory, this process has 13.82 GiB memory in use. Of the allocated memory 13.22 GiB is allocated by PyTorch, and 15.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = sft_model,\n",
    "    fast_inference = False,\n",
    "    load_in_4bit = False,\n",
    "    max_seq_length = None,\n",
    "    gpu_memory_utilization = 0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040fa6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "def format_input_prompt(system_message, user_input):\n",
    "    formatted_input = [\n",
    "        {\"role\": \"assistant\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    return formatted_input\n",
    "\n",
    "def format_validation_example_for_inference(example):\n",
    "    return example.split(\"<|start_header_id|>user<|end_header_id|>\")[1].split(\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\")[0]\n",
    "\n",
    "def inference(model, system_message, user_input, max_new_tokens=None, **kwargs):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        format_input_prompt(system_message, user_input),\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors = \"pt\").to(\"cuda\")\n",
    "    if not max_new_tokens:\n",
    "        max_new_tokens = model.config.max_position_embeddings - input_ids.shape[-1]\n",
    "    model.generate(input_ids, streamer = text_streamer, max_new_tokens=max_new_tokens, **kwargs)\n",
    "\n",
    "def predict(model, system_message, user_input, max_new_tokens=None, **kwargs):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        format_input_prompt(system_message, user_input),\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors = \"pt\").to(\"cuda\")\n",
    "    if not max_new_tokens:\n",
    "        max_new_tokens = model.config.max_position_embeddings - input_ids.shape[-1]\n",
    "    \n",
    "    output_ids = model.generate(input_ids, max_new_tokens=max_new_tokens, **kwargs)\n",
    "    result = tokenizer.batch_decode(output_ids)\n",
    "    processed_result = result[0].split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1].split(\"<|eot_id|>\")[0]\n",
    "    return processed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path:str, filename:str):\n",
    "    with open(os.path.join(path, filename), mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def format_example(example:dict, system_message):\n",
    "        formatted_example = [\n",
    "            {\"role\": \"assistant\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps(example[\"output\"])}\n",
    "        ]\n",
    "        return formatted_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abbeda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"/mnt/data/openCTI/splitted-io-pairs/test\"\n",
    "inputs = []\n",
    "outputs = []\n",
    "include_cti_type = [\"malware\"]\n",
    "\n",
    "for file in os.listdir(test_path):\n",
    "    cti_type = file.split(\"--\")[0]\n",
    "    if cti_type not in include_cti_type:\n",
    "        continue\n",
    "    example = load_json(test_path, file)\n",
    "    inputs.append(example[\"input\"])\n",
    "    outputs.append(example[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b2216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf87972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"objects\": [{\"id\": \"\", \"type\": \"malware\", \"name\": \"BlueSky\", \"is_family\": false}]}\n"
     ]
    }
   ],
   "source": [
    "system_message = sft_system_message\n",
    "user_input = inputs[3]\n",
    "inference(model,\n",
    "          system_message, \n",
    "          user_input, \n",
    "          max_new_tokens=500,\n",
    "          temperature=0.9,\n",
    "          top_p=0.9,\n",
    "          repetition_penalty=1.1,\n",
    "          no_repeat_ngram_size=3,\n",
    "          do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e29f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "LlamaForCausalLM has no `_prepare_4d_causal_attention_mask_with_cache_position` method defined in its base modeling class. Compiled forward passes will be sub-optimal. If you're writing code, see Llama for an example implementation. If you're a user, please report this issue on GitHub.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:40<00:00,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "system_message = sft_system_message\n",
    "inputs = inputs\n",
    "outputs = outputs\n",
    "\n",
    "preds = [predict(model,\n",
    "                 system_message,\n",
    "                 user_input,\n",
    "                 max_new_tokens=500,\n",
    "                 temperature=0.6,\n",
    "                 top_p=0.2,\n",
    "                 repetition_penalty=1.1,\n",
    "                 no_repeat_ngram_size=3,\n",
    "                 do_sample=True) for user_input in tqdm(inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ae5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds4eval = []\n",
    "failed_preds = []\n",
    "pattern = r'\\{\\s*\"id\"\\s*:\\s*\"[^\"]*\"\\s*,\\s*\"type\"\\s*:\\s*\"[^\"]*\"\\s*,\\s*\"name\"\\s*:\\s*\"[^\"]*\"\\s*,\\s*\"is_family\"\\s*:\\s*(?:true|false|null|\"(?:[^\"]*)\"|[-+]?\\d+(?:\\.\\d+)?)\\s*\\}'\n",
    "\n",
    "for p in preds:\n",
    "    try:\n",
    "        preds4eval.append(\n",
    "            {\n",
    "                \"objects\":json.loads(p.lower())[\"objects\"]\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        objects = re.findall(pattern, p)\n",
    "        if not objects:\n",
    "            failed_preds.append(p)\n",
    "        else:\n",
    "            valid_objs = [json.loads(obj) for obj in objects]\n",
    "        preds4eval.append(\n",
    "            {\n",
    "                \"objects\":valid_objs\n",
    "                }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26447f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percenrage of failed json outputs: 14.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percenrage of failed json outputs: {'{:.1f}'.format(100 * len(failed_preds) / len(inputs))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b613fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing\n",
    "processed_preds4eval = []\n",
    "\n",
    "def fix_malware_id(wrong_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Fixes malformed malware IDs according to the observed patterns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Trim spaces\n",
    "    s = wrong_id.strip().lower()\n",
    "\n",
    "    # Remove leading underscores or hyphens\n",
    "    s = re.sub(r'^[-_]+', '', s)\n",
    "\n",
    "    # Remove redundant 'malware' if it‚Äôs at the start but malformed\n",
    "    s = re.sub(r'^(malware[-_]+)', '', s)\n",
    "\n",
    "    # Handle duplicated name parts (e.g., 'fatboy--fatboy')\n",
    "    parts = re.split(r'--+', s)\n",
    "    if len(parts) == 2 and parts[0] == parts[1]:\n",
    "        s = parts[0]\n",
    "\n",
    "    # Prepend 'malware--'\n",
    "    corrected = f\"malware--{s}\"\n",
    "\n",
    "    # Ensure only one double dash after 'malware'\n",
    "    corrected = re.sub(r'^malware-+', 'malware--', corrected)\n",
    "\n",
    "    return corrected\n",
    "\n",
    "# Step 1\n",
    "for p in preds4eval:\n",
    "    objects = []\n",
    "    for obj in p[\"objects\"]:\n",
    "        # Step 1\n",
    "        if \"id\" in obj.keys():\n",
    "            ID = fix_malware_id(obj[\"id\"].strip())\n",
    "            NAME = ID.split(\"malware--\")[-1]\n",
    "\n",
    "        # if \"name\" in obj.keys():\n",
    "        #     NAME = obj[\"name\"]\n",
    "\n",
    "        if \"is_family\" in obj.keys():\n",
    "            IS_FAMILY = obj[\"is_family\"]\n",
    "        else:\n",
    "            IS_FAMILY = False\n",
    "\n",
    "        objects.append(\n",
    "            {\n",
    "                \"id\":ID,\n",
    "                \"type\":\"malware\",\n",
    "                \"name\":NAME,\n",
    "                \"is_family\":IS_FAMILY\n",
    "            }\n",
    "        )\n",
    "\n",
    "    processed_preds4eval.append(\n",
    "                {\n",
    "                    \"objects\":objects\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badff28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.stix_evaluator import STIXEvaluator\n",
    "\n",
    "evaluator = STIXEvaluator(comparison_values=[\"id\"], cti_object_types=[\"malware\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f2ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison: 0.56757\n",
      "Recall: 0.32812\n",
      "F1-Score: 0.41584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deleftheriou/cti-model-training/evaluation/stix_evaluator.py:271: STIXWarning: \n",
      "Type and Name will be used to compare stix objects!\n",
      "  warnings.warn(\"\\nType and Name will be used to compare stix objects!\", STIXWarning)\n",
      "/home/deleftheriou/cti-model-training/evaluation/stix_evaluator.py:276: STIXWarning: \n",
      "All cti types will be evaluated!\n",
      "  warnings.warn(\"\\nAll cti types will be evaluated!\", STIXWarning)\n"
     ]
    }
   ],
   "source": [
    "p, r, f1, full_res = evaluator._evaluate_(predicted=processed_preds4eval, actual=outputs)\n",
    "print(f\"Precison: {p}\\nRecall: {r}\\nF1-Score: {f1}\")\n",
    "\n",
    "# Temperature 0.2\n",
    "# Min_p 0.2\n",
    "# Precison: 0.7\n",
    "# Recall: 0.4375\n",
    "# F1-Score: 0.53846\n",
    "\n",
    "# Temperature 0.2\n",
    "# Min_p 0.1\n",
    "# Precison: 0.7\n",
    "# Recall: 0.4375\n",
    "# F1-Score: 0.53846\n",
    "\n",
    "# Temperature 0.7\n",
    "# Min_p 0.2\n",
    "# Precison: 0.7\n",
    "# Recall: 0.4375\n",
    "# F1-Score: 0.53846\n",
    "\n",
    "# Temperature 0.1\n",
    "# Min_p 0.1\n",
    "# Precison: 0.7\n",
    "# Recall: 0.4375\n",
    "# F1-Score: 0.53846\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Detailed results:\\n {full_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77107ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72e6ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model=\"gpt-oss:120b\",\n",
    "                  num_ctx=128000,\n",
    "                  num_predict=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b4b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 6/307 [01:51<1:09:43, 13.90s/it]"
     ]
    }
   ],
   "source": [
    "preds = [model.invoke(sft_system_message + user_input) for user_input in tqdm.tqdm(inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60d4af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds4eval = []\n",
    "failed_preds = []\n",
    "pattern = r'\\{\\s*\"id\"\\s*:\\s*\"[^\"]*\"\\s*,\\s*\"type\"\\s*:\\s*\"[^\"]*\"\\s*,\\s*\"name\"\\s*:\\s*\"[^\"]*\"\\s*,\\s*\"is_family\"\\s*:\\s*(?:true|false|null|\"(?:[^\"]*)\"|[-+]?\\d+(?:\\.\\d+)?)\\s*\\}'\n",
    "\n",
    "for p in preds:\n",
    "\n",
    "    p = p.lower().replace(\"\\t\", \"\")\n",
    "\n",
    "    try:\n",
    "        objects = json.loads(p)[\"objects\"]\n",
    "    except:\n",
    "        objects = re.findall(pattern, p)\n",
    "        if not objects and '\"objects\": []' not in p:\n",
    "            failed_preds.append(p)\n",
    "        elif objects:\n",
    "            objects = [json.loads(obj) for obj in objects]\n",
    "    \n",
    "    preds4eval.append(\n",
    "            {\n",
    "                \"objects\":objects\n",
    "                }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "293130d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percenrage of failed json outputs: 2.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percenrage of failed json outputs: {'{:.1f}'.format(100 * len(failed_preds) / len(inputs))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing\n",
    "processed_preds4eval = []\n",
    "\n",
    "def fix_malware_id(wrong_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Fixes malformed malware IDs according to the observed patterns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Trim spaces\n",
    "    s = wrong_id.strip().lower()\n",
    "\n",
    "    # Remove leading underscores or hyphens\n",
    "    s = re.sub(r'^[-_]+', '', s)\n",
    "\n",
    "    # Remove redundant 'malware' if it‚Äôs at the start but malformed\n",
    "    s = re.sub(r'^(malware[-_]+)', '', s)\n",
    "\n",
    "    # Handle duplicated name parts (e.g., 'fatboy--fatboy')\n",
    "    parts = re.split(r'--+', s)\n",
    "    if len(parts) == 2 and parts[0] == parts[1]:\n",
    "        s = parts[0]\n",
    "\n",
    "    # Prepend 'malware--'\n",
    "    corrected = f\"malware--{s}\"\n",
    "\n",
    "    # Ensure only one double dash after 'malware'\n",
    "    corrected = re.sub(r'^malware-+', 'malware--', corrected)\n",
    "\n",
    "    return corrected\n",
    "\n",
    "# Step 1\n",
    "for p in preds4eval:\n",
    "    objects = []\n",
    "    for obj in p[\"objects\"]:\n",
    "        # Step 1\n",
    "        if \"id\" in obj.keys():\n",
    "            ID = fix_malware_id(obj[\"id\"].strip())\n",
    "\n",
    "        if \"name\" in obj.keys():\n",
    "            NAME = obj[\"name\"]\n",
    "        else:\n",
    "            NAME = ID.split(\"malware--\")[-1]\n",
    "\n",
    "        if \"is_family\" in obj.keys():\n",
    "            IS_FAMILY = obj[\"is_family\"]\n",
    "        else:\n",
    "            IS_FAMILY = False\n",
    "\n",
    "        objects.append(\n",
    "            {\n",
    "                \"id\":ID,\n",
    "                \"type\":\"malware\",\n",
    "                \"name\":NAME,\n",
    "                \"is_family\":IS_FAMILY\n",
    "            }\n",
    "        )\n",
    "\n",
    "    processed_preds4eval.append(\n",
    "                {\n",
    "                    \"objects\":objects\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "575073ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.stix_evaluator import STIXEvaluator\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "evaluator = STIXEvaluator(comparison_values=[\"type\", \"name\"], cti_object_types=[\"malware\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a4abd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison: 0.24587\n",
      "Recall: 0.71401\n",
      "F1-Score: 0.36578\n"
     ]
    }
   ],
   "source": [
    "p, r, f1, full_res = evaluator._evaluate_(predicted=processed_preds4eval, actual=outputs)\n",
    "print(f\"Precison: {p}\\nRecall: {r}\\nF1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e82755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
