{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0599d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f48e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_references = pd.read_csv(\"data/opencti_reports_external_references.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7034dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_dict = {\n",
    "    x[\"ID\"]: ast.literal_eval(x[\"Ext. Reference URLs\"])\n",
    "    for _, x in reports_references.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4e4da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AIAS\\Documents\\cti-model-training\\venv-eda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "converter = DocumentConverter()\n",
    "\n",
    "def extract_report(source):\n",
    "    result = converter.convert(source)\n",
    "    try:\n",
    "        # Extract image URLs\n",
    "        response = requests.get(source)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        image_urls = [img['src'] for img in soup.find_all('img') if 'src' in img.attrs]\n",
    "    except:\n",
    "        image_urls = []\n",
    "    \n",
    "    image_urls = \"\\n\".join(image_urls)\n",
    "\n",
    "    if image_urls:\n",
    "        final_result = f\"{result.document.export_to_markdown()}\\n\\n# Image URLs #\\n\\n{image_urls}\"\n",
    "    else:\n",
    "        final_result = result.document.export_to_markdown()\n",
    "\n",
    "    return final_result\n",
    "\n",
    "def get_soup_object(url, use_table_counter=True):\n",
    "    try:\n",
    "        options = Options()\n",
    "        #options.add_argument('--headless')  # Run in headless mode (no window)\n",
    "        options.add_argument('--disable-gpu')\n",
    "        \n",
    "        # 1. Initialize Selenium WebDriver (adjust path to your webdriver)\n",
    "        driver = webdriver.Chrome(options=options)  # Or any other browser driver\n",
    "        \n",
    "        # 2. Navigate to the webpage containing the table\n",
    "        driver.get(url)\n",
    "\n",
    "        if use_table_counter:\n",
    "\n",
    "            # # 3. Locate the <select> element that controls the number of items\n",
    "            # select_element = WebDriverWait(driver, 10).until(\n",
    "            #     EC.presence_of_element_located((By.ID, \"count\"))  # Replace with the actual ID\n",
    "            #     # Or use other locators like By.NAME, By.XPATH, By.CSS_SELECTOR\n",
    "            # )\n",
    "\n",
    "            select_element = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.ID, \"count\"))\n",
    "            )\n",
    "            \n",
    "            item_selector = Select(select_element)\n",
    "\n",
    "            # 4. Select the desired number of items\n",
    "            desired_item_count = \"100\"  # Replace with the value you want to select (e.g., \"50\", \"All\")\n",
    "            item_selector.select_by_value(desired_item_count)  # Or select_by_visible_text() or select_by_index()\n",
    "\n",
    "            # 5. Wait for the table to load/update after the selection (important!)\n",
    "            WebDriverWait(driver, 10)\n",
    "            # .until(\n",
    "            #     EC.presence_of_element_located((By.XPATH, \"//table[@id='your_table_id']/tbody/tr\"))  # Adjust XPath to target a row in the table\n",
    "            # )\n",
    "            time.sleep(2)  # Add an extra small delay if needed for rendering\n",
    "\n",
    "        # 6. Get the updated HTML source\n",
    "        html_source = driver.page_source\n",
    "        soup = BeautifulSoup(html_source, 'html.parser')\n",
    "    finally:\n",
    "        # 8. Close the browser\n",
    "        driver.quit()\n",
    "    return soup\n",
    "\n",
    "def extract_alienvault_report(url, use_table_counter=True):\n",
    "\n",
    "    soup = get_soup_object(url, use_table_counter)\n",
    "    \n",
    "    # Get title\n",
    "    title = soup.find(\"h1\").get_text()\n",
    "\n",
    "    # Get metadata\n",
    "    metadata = soup.find(\"div\", class_=\"pulse-meta\").find('ul').get_text().strip()\n",
    "\n",
    "    # Get contents\n",
    "    contents = \"\\n\\n\".join(\n",
    "        [tag.get_text().strip() for tag in soup.find(\"div\", class_=\"pulse-general-details\").find_all(\"div\", class_=\"row col-md-12 col-flex detail-row ng-star-inserted\")]\n",
    "        )\n",
    "\n",
    "    # Get table\n",
    "    table_rows = soup.find(\"otx-table\").find_all(\"tr\")\n",
    "\n",
    "    table_dict = {}\n",
    "    for i, tr in enumerate(table_rows):\n",
    "        if i==0:\n",
    "            table_columns = []\n",
    "            for tag in tr.find_all(\"div\", class_=\"clickable\"):\n",
    "                table_dict.update({tag.get_text().strip():[]})\n",
    "                table_columns.append(tag.get_text().strip())\n",
    "        else:\n",
    "            for j, tag in enumerate(tr.find_all(\"td\", class_=\"ng-star-inserted\")):\n",
    "                k = table_columns[j]\n",
    "                table_dict[k].append(tag.get_text().strip())\n",
    "\n",
    "    if \"\" in table_dict.keys():\n",
    "        table_dict.pop(\"\")\n",
    "\n",
    "    table_str = pd.DataFrame(table_dict).to_string(index=False, header=True, na_rep='--', col_space=2)\n",
    "\n",
    "    alienvault_report = \"# Title #\\n{title}\\n\\n# Metadata #\\n{metadata}\\n\\n# Contents #\\n{contents}\\n\\n# Indicators of Compromise #\\n{table}\".format(\n",
    "        title = title,\n",
    "        metadata = metadata,\n",
    "        contents = contents,\n",
    "        table = table_str\n",
    "    )\n",
    "    return alienvault_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb1c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "for k, v in tqdm.tqdm(references_dict.items()):\n",
    "\n",
    "    doc = \"\"\n",
    "\n",
    "    raw_path = os.path.join(\"data/reports_formatted/raw\", k)\n",
    "    alienvault_path = os.path.join(\"data/reports_formatted/alienvault\", k)\n",
    "\n",
    "    paths_exist = 0\n",
    "    for p in [raw_path, alienvault_path]:\n",
    "        if os.path.exists(p):\n",
    "            paths_exist += 1\n",
    "            continue\n",
    "        else:\n",
    "            os.mkdir(p)\n",
    "\n",
    "    if paths_exist==2:\n",
    "        continue\n",
    "\n",
    "    for i, url in enumerate(v):\n",
    "\n",
    "        if url.startswith(\"https://otx.alienvault.com/\"):\n",
    "            save_path = f\"data/reports_formatted/alienvault/{k}\"\n",
    "            extract = extract_alienvault_report\n",
    "        else:\n",
    "            save_path = f\"data/reports_formatted/raw/{k}\"\n",
    "            extract = extract_report\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(extract, url)\n",
    "\n",
    "            try:\n",
    "                report = future.result(timeout=15)\n",
    "            except Exception as e:\n",
    "                report = \"REPORT IS NOT EXTRACTED! Reason caused the failure:\\n\\n{e}\".format(\n",
    "                    e = e\n",
    "                )\n",
    "        doc = \"External reference URL: {url}\\n\\nCTI REPORT\\n\\n{report}\\n\\n\".format(\n",
    "            url = url,\n",
    "            report = report\n",
    "        )\n",
    "\n",
    "        with open(os.path.join(save_path, f\"ref_{i}\"), mode=\"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
