{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab2e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-07 10:55:25 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "num_proc = cpu_count()\n",
    "\n",
    "import yaml\n",
    "\n",
    "from dataprep.stix.StixConfig import StixToPydanticMap, STIX\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "\n",
    "from evaluation.stix_evaluator import STIXEvaluator\n",
    "\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import torch\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "from data_processor import SplittedJsonIoDataset\n",
    "from customs import customize_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee74d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e89ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"grpo_config.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac42faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.8: Fast Llama patching. Transformers: 4.53.0. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.19 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/llama-3-8b-instruct-bnb-4bit with actual GPU utilization = 69.53%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.19 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 8192. Num Sequences = 368.\n",
      "Unsloth: vLLM's KV Cache can use up to 48.73 GB. Also swap space = 6 GB.\n",
      "INFO 07-07 10:56:04 [config.py:823] This model supports multiple tasks: {'generate', 'classify', 'embed', 'score', 'reward'}. Defaulting to 'generate'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cab0cfbef048f290aca10ab587533e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 10:56:04 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d159fe9ac94fe69e9bd18f5764c71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0890af8289b143ef86214edcb06b8710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0898487f932d44e0bee73899c158db4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 10:56:07 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='unsloth/llama-3-8b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/llama-3-8b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/llama-3-8b-instruct-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 07-07 10:56:07 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f3b6beac550>\n",
      "INFO 07-07 10:56:08 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-07 10:56:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-07 10:56:08 [gpu_model_runner.py:1595] Starting to load model unsloth/llama-3-8b-instruct-bnb-4bit...\n",
      "INFO 07-07 10:56:08 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 07-07 10:56:08 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-07 10:56:08 [bitsandbytes_loader.py:454] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 07-07 10:56:09 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c71c4d99fb8419bba67eddfe20aa513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 10:56:25 [weight_utils.py:308] Time spent downloading weights for unsloth/llama-3-8b-instruct-bnb-4bit: 16.196222 seconds\n",
      "INFO 07-07 10:56:25 [weight_utils.py:345] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109c356edaf74d8698550b00e5b0d319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f35be8963a4a24b5165e5c645437a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 10:56:26 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 07-07 10:56:27 [gpu_model_runner.py:1624] Model loading took 5.6733 GiB and 18.317503 seconds\n",
      "INFO 07-07 10:56:38 [backends.py:462] Using cache directory: /home/deleftheriou/.cache/vllm/torch_compile_cache/085dc06a26/rank_0_0 for vLLM's torch.compile\n",
      "INFO 07-07 10:56:38 [backends.py:472] Dynamo bytecode transform time: 10.27 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  6.11it/s, triton_poi_fused_cat_5]                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 10:56:41 [backends.py:161] Cache the graph of shape None for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.12it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 62.06it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 60.49it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 219.61it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 189.30it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 219.04it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 151.88it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 127.24it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 135.72it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 129.32it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 142.36it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 139.27it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 194.77it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 120.03it/s, triton_poi_fused_cat_7]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 185.77it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 135.20it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 153.38it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 200.17it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 181.95it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 140.59it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 196.50it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 144.23it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 146.89it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 231.89it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 220.63it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 220.06it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 174.13it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 212.83it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 132.43it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 123.11it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 130.18it/s, triton_poi_fused_cat_7]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12.55it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 10:57:10 [backends.py:173] Compiling a graph for general shape takes 30.65 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 10:58:17 [monitor.py:34] torch.compile takes 40.92 s in total\n",
      "INFO 07-07 10:58:20 [gpu_worker.py:227] Available KV cache memory: 47.50 GiB\n",
      "INFO 07-07 10:58:21 [kv_cache_utils.py:715] GPU KV cache size: 389,104 tokens\n",
      "INFO 07-07 10:58:21 [kv_cache_utils.py:719] Maximum concurrency for 8,192 tokens per request: 47.50x\n",
      "INFO 07-07 10:59:30 [gpu_model_runner.py:2048] Graph capturing finished in 69 secs, took 1.60 GiB\n",
      "INFO 07-07 10:59:30 [core.py:171] init engine (profile, create kv cache, warmup model) took 182.64 seconds\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'k_norm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'k_norm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm']\n",
      "unsloth/llama-3-8b-instruct-bnb-4bit does not have a padding token! Will use pad_token = <|reserved_special_token_250|>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# # Check if model is a peft model\n",
    "# import peft.helpers\n",
    "\n",
    "# def load_model_and_tokenizer(model_name_or_path, config, **kwargs):\n",
    "#     if peft.helpers.check_if_peft_model(model_name_or_path):\n",
    "#         model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#                 model_name=model_name_or_path,\n",
    "#                 **kwargs\n",
    "#             )\n",
    "#         if config[\"merge_peft_model\"]:\n",
    "#             model.merge_and_unload()\n",
    "#     else:\n",
    "#         model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#             **config[\"model_loading_args\"]\n",
    "#         )\n",
    "#     return model, tokenizer\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/mnt/data/training-outputs/first-run/outputs/checkpoint-194\",\n",
    "    fast_inference = True,\n",
    "    load_in_4bit = True,\n",
    "    max_seq_length = None,\n",
    "    gpu_memory_utilization = 0.7\n",
    "    #65536\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a38459cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b869905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 8,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b75b0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, tokenizer = customize_tokenizer(model, tokenizer, config)\n",
    "# no need for deepseek\n",
    "# config[\"chat_template\"] = \"deepseek\"\n",
    "# tokenizer = get_chat_template(tokenizer, config[\"chat_template\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7c688ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1dcb33d7c745d196f512a2994db31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1564 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0d9396f304488d9b9dc5356d288445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dataset for training\n",
    "dataset = SplittedJsonIoDataset(tokenizer, config).grpo_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72a82611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_answer(answer: str) -> dict:\n",
    "    return json.loads(answer)\n",
    "\n",
    "def deserialize_response_for_evaluation(answer: str) -> dict:\n",
    "    if is_stix_bundle(answer):\n",
    "        return json.loads(answer)\n",
    "    else:\n",
    "        return {\"id\":\"\", \"type\":\"bundle\", \"objects\":[]}\n",
    "\n",
    "def extract_xml_answer(response: str) -> str:\n",
    "    answer = response.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def is_stix_bundle(text: str) -> bool:\n",
    "    try:\n",
    "        bundle = json.loads(text)\n",
    "        pydantic_stix_bundle = STIX(**bundle)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def count_valid_stix_objects(text: str) -> bool:\n",
    "    smap = StixToPydanticMap()\n",
    "    cnt = 0.0\n",
    "    if is_stix_bundle(text):\n",
    "        bundle = json.loads(text)\n",
    "        for obj in bundle[\"objects\"]:\n",
    "            try:\n",
    "                smap(obj)\n",
    "                cnt += 1\n",
    "            except ValidationError:\n",
    "                pass\n",
    "        return cnt / len(bundle[\"objects\"])\n",
    "    else:\n",
    "      return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d39fdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<think>.*?</think>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def stix_validity_reward_func(completions, answers, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion can is a stix bundle.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if is_stix_bundle(r) else 0.0 for r in extracted_responses]\n",
    "\n",
    "def stix_objects_validity_reward_func(completions, answers, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has valid stix objects.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 * count_valid_stix_objects(r) for r in extracted_responses]\n",
    "\n",
    "def accuracy_reward_func(completions, answers, **kwargs) -> list[float]:\n",
    "    evaluator = STIXEvaluator()\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    desirialized_responses = [deserialize_response_for_evaluation(r) for r in extracted_responses]\n",
    "    desirialized_answers = [deserialize_answer(a) for a in answers]\n",
    "    return [evaluator.evaluate_single(r, a)[2] for r, a in zip(desirialized_responses, desirialized_answers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a0d8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 4\n"
     ]
    }
   ],
   "source": [
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference!\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = 16384, #config[\"model_loading_args\"][\"max_seq_length\"],\n",
    "    max_completion_length = 16384, #config[\"model_loading_args\"][\"max_seq_length\"],\n",
    "    num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    save_steps = 250,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"tensorboard\", # Can use Weights & Biases\n",
    "    output_dir = \"grpo_outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00505a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        format_reward_func,\n",
    "        stix_validity_reward_func,\n",
    "        stix_objects_validity_reward_func,\n",
    "        accuracy_reward_func\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    #eval_dataset = dataset[\"eval\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3142d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,564 | Num Epochs = 1 | Total steps = 1,564\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 20,971,520/8,000,000,000 (0.26% trained)\n"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "# Start training\n",
    "trainer.train()\n",
    "#trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891aa65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
