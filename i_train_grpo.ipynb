{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab2e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-03 12:27:57 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "num_proc = cpu_count()\n",
    "\n",
    "import yaml\n",
    "\n",
    "from dataprep.stix.StixConfig import StixToPydanticMap, STIX\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "\n",
    "from evaluation.stix_evaluator import STIXEvaluator\n",
    "\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import torch\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "from data_processor import SplittedJsonIoDataset\n",
    "from customs import customize_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee74d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e89ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"grpo_config.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac42faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.8: Fast Qwen3 patching. Transformers: 4.53.0. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.19 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: vLLM loading unsloth/deepseek-r1-0528-qwen3-8b-unsloth-bnb-4bit with actual GPU utilization = 49.66%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.19 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 65536. Num Sequences = 320.\n",
      "Unsloth: vLLM's KV Cache can use up to 32.64 GB. Also swap space = 6 GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-03 13:05:56 [config.py:823] This model supports multiple tasks: {'reward', 'generate', 'classify', 'score', 'embed'}. Defaulting to 'generate'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c4fd50f5ef49b1aa0906ff5260e752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-03 13:05:56 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=65536.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.33.self_attn', 'model.layers.34.self_attn', 'model.layers.1.self_attn', 'model.layers.6.self_attn', 'model.layers.34.mlp', 'model.layers.4.mlp', 'model.layers.2.mlp', 'model.layers.5.mlp', 'model.layers.6.mlp'], 'llm_int8_threshold': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6928ad91793455dbce32d6412db8fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69025943ae0e443fbd1bc4103f1c5091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fe31ddd5f54ba29c50a9be8b767898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31054b5d397a454ea1b4de8ffb9970c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/171 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-03 13:06:00 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='unsloth/deepseek-r1-0528-qwen3-8b-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/deepseek-r1-0528-qwen3-8b-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/deepseek-r1-0528-qwen3-8b-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 07-03 13:06:00 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f072ea4e450>\n",
      "INFO 07-03 13:06:01 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-03 13:06:01 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-03 13:06:01 [gpu_model_runner.py:1595] Starting to load model unsloth/deepseek-r1-0528-qwen3-8b-unsloth-bnb-4bit...\n",
      "INFO 07-03 13:06:01 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 07-03 13:06:01 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-03 13:06:01 [bitsandbytes_loader.py:454] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 07-03 13:06:02 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ee5f51a0814984825c6e4b2c2b2a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6211293203b46c48184440e311200fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-03 13:06:15 [weight_utils.py:308] Time spent downloading weights for unsloth/deepseek-r1-0528-qwen3-8b-unsloth-bnb-4bit: 13.440408 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee73188fcb94451bab72afda0f547704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7115adaf172644cf98cafef790b068f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8e113bc08446bf9e7d05d8b3726baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-03 13:06:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 07-03 13:06:17 [gpu_model_runner.py:1624] Model loading took 7.3485 GiB and 15.758609 seconds\n",
      "INFO 07-03 13:06:28 [backends.py:462] Using cache directory: /home/deleftheriou/.cache/vllm/torch_compile_cache/56d0d6642f/rank_0_0 for vLLM's torch.compile\n",
      "INFO 07-03 13:06:28 [backends.py:472] Dynamo bytecode transform time: 10.42 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  4.75it/s, triton_poi_fused_add_mul_sub_5]                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-03 13:06:32 [backends.py:161] Cache the graph of shape None for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 12.55it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 111.52it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 216.20it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 213.56it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 160.54it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 218.64it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 218.31it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 104.86it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 106.74it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 106.33it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 167.55it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 165.36it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 169.75it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 157.97it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 164.33it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 67.90it/s, triton_poi_fused_add_mul_sub_9]                    \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 101.13it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 157.69it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 102.38it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 158.51it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 50.32it/s, triton_poi_fused_add_mul_sub_9]                    \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 148.15it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 163.78it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 192.92it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 62.02it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 100.33it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 170.70it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 66.85it/s, triton_poi_fused_add_mul_sub_9]                    \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 71.25it/s, triton_poi_fused_add_mul_sub_9]                    \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 173.52it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 101.77it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 58.23it/s, triton_poi_fused_add_mul_sub_9]                    \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 66.44it/s, triton_poi_fused_add_mul_sub_9]                    \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 212.68it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 161.83it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 16.64it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-03 13:07:08 [backends.py:173] Compiling a graph for general shape takes 38.33 s\n",
      "INFO 07-03 13:09:08 [monitor.py:34] torch.compile takes 48.75 s in total\n",
      "INFO 07-03 13:09:11 [gpu_worker.py:227] Available KV cache memory: 24.07 GiB\n",
      "INFO 07-03 13:09:11 [kv_cache_utils.py:715] GPU KV cache size: 175,264 tokens\n",
      "INFO 07-03 13:09:11 [kv_cache_utils.py:719] Maximum concurrency for 65,536 tokens per request: 2.67x\n",
      "INFO 07-03 13:10:16 [gpu_model_runner.py:2048] Graph capturing finished in 65 secs, took 1.71 GiB\n",
      "INFO 07-03 13:10:17 [core.py:171] init engine (profile, create kv cache, warmup model) took 239.11 seconds\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm']\n"
     ]
    }
   ],
   "source": [
    "# # Check if model is a peft model\n",
    "# import peft.helpers\n",
    "\n",
    "# def load_model_and_tokenizer(model_name_or_path, config, **kwargs):\n",
    "#     if peft.helpers.check_if_peft_model(model_name_or_path):\n",
    "#         model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#                 model_name=model_name_or_path,\n",
    "#                 **kwargs\n",
    "#             )\n",
    "#         if config[\"merge_peft_model\"]:\n",
    "#             model.merge_and_unload()\n",
    "#     else:\n",
    "#         model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#             **config[\"model_loading_args\"]\n",
    "#         )\n",
    "#     return model, tokenizer\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\", #\"/mnt/data/training-outputs/first-run/outputs/checkpoint-194\",\n",
    "    fast_inference = True,\n",
    "    max_seq_length = 65536\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b869905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.8 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 8,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, tokenizer = customize_tokenizer(model, tokenizer, config)\n",
    "# no need for deepseek\n",
    "# config[\"chat_template\"] = \"deepseek\"\n",
    "# tokenizer = get_chat_template(tokenizer, config[\"chat_template\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c688ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efb7f1476d84fe28335a0de97424c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1564 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254251470ca9456e9671bc805b49c2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dataset for training\n",
    "dataset = SplittedJsonIoDataset(tokenizer, config).grpo_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72a82611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_answer(answer: str) -> dict:\n",
    "    return json.loads(answer)\n",
    "\n",
    "def deserialize_response_for_evaluation(answer: str) -> dict:\n",
    "    if is_stix_bundle(answer):\n",
    "        return json.loads(answer)\n",
    "    else:\n",
    "        return {\"id\":\"\", \"type\":\"bundle\", \"objects\":[]}\n",
    "\n",
    "def extract_xml_answer(response: str) -> str:\n",
    "    answer = response.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def is_stix_bundle(text: str) -> bool:\n",
    "    try:\n",
    "        bundle = json.loads(text)\n",
    "        pydantic_stix_bundle = STIX(**bundle)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def count_valid_stix_objects(text: str) -> bool:\n",
    "    smap = StixToPydanticMap()\n",
    "    cnt = 0.0\n",
    "    if is_stix_bundle(text):\n",
    "        bundle = json.loads(text)\n",
    "        for obj in bundle[\"objects\"]:\n",
    "            try:\n",
    "                smap(obj)\n",
    "                cnt += 1\n",
    "            except ValidationError:\n",
    "                pass\n",
    "        return cnt / len(bundle[\"objects\"])\n",
    "    else:\n",
    "      return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d39fdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def stix_validity_reward_func(completions, answers, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion can is a stix bundle.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if is_stix_bundle(r) else 0.0 for r in extracted_responses]\n",
    "\n",
    "def stix_objects_validity_reward_func(completions, answers, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has valid stix objects.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 * count_valid_stix_objects(r) for r in extracted_responses]\n",
    "\n",
    "def accuracy_reward_func(completions, answers, **kwargs) -> list[float]:\n",
    "    evaluator = STIXEvaluator()\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    desirialized_responses = [deserialize_response_for_evaluation(r) for r in extracted_responses]\n",
    "    desirialized_answers = [deserialize_answer(a) for a in answers]\n",
    "    return [evaluator.evaluate_single(r, a)[2] for r, a in zip(desirialized_responses, desirialized_answers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7a0d8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 4 to the `num_generations` of 8\n"
     ]
    }
   ],
   "source": [
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference!\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 16, # Increase to 4 for smoother training\n",
    "    num_generations = 8, # Decrease if out of memory\n",
    "    max_prompt_length = config[\"model_loading_args\"][\"max_seq_length\"],\n",
    "    max_completion_length = config[\"model_loading_args\"][\"max_seq_length\"],\n",
    "    num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    save_steps = 250,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"tensorboard\", # Can use Weights & Biases\n",
    "    output_dir = \"grpo_outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00505a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        format_reward_func,\n",
    "        stix_validity_reward_func,\n",
    "        stix_objects_validity_reward_func,\n",
    "        accuracy_reward_func\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"eval\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3142d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,564 | Num Epochs = 1 | Total steps = 98\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 16 x 1) = 128\n",
      " \"-____-\"     Trainable parameters = 21,823,488/8,000,000,000 (0.27% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 07-03 13:17:23 [dump_input.py:69] Dumping input data\n",
      "ERROR 07-03 13:17:23 [dump_input.py:71] V1 LLM engine (v0.9.1) with config: model='unsloth/deepseek-r1-0528-qwen3-8b-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/deepseek-r1-0528-qwen3-8b-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/deepseek-r1-0528-qwen3-8b-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"/home/deleftheriou/.cache/vllm/torch_compile_cache/56d0d6642f\",\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":\"/home/deleftheriou/.cache/vllm/torch_compile_cache/56d0d6642f/rank_0_0\"}, \n",
      "ERROR 07-03 13:17:23 [dump_input.py:79] Dumping scheduler output for model execution:\n",
      "ERROR 07-03 13:17:23 [dump_input.py:80] SchedulerOutput(scheduled_new_reqs=[], scheduled_cached_reqs=[CachedRequestData(req_id='0', resumed_from_preemption=false, new_token_ids=[21721], new_block_ids=[[8833]], num_computed_tokens=7232), CachedRequestData(req_id='1', resumed_from_preemption=false, new_token_ids=[4220], new_block_ids=[[8832]], num_computed_tokens=7232), CachedRequestData(req_id='2', resumed_from_preemption=false, new_token_ids=[5396], new_block_ids=[[8831]], num_computed_tokens=7232), CachedRequestData(req_id='3', resumed_from_preemption=false, new_token_ids=[264], new_block_ids=[[8830]], num_computed_tokens=7232), CachedRequestData(req_id='4', resumed_from_preemption=false, new_token_ids=[82738], new_block_ids=[[8829]], num_computed_tokens=7232), CachedRequestData(req_id='5', resumed_from_preemption=false, new_token_ids=[42052], new_block_ids=[[8828]], num_computed_tokens=7232), CachedRequestData(req_id='6', resumed_from_preemption=false, new_token_ids=[1034], new_block_ids=[[8827]], num_computed_tokens=7232), CachedRequestData(req_id='7', resumed_from_preemption=false, new_token_ids=[7531], new_block_ids=[[8826]], num_computed_tokens=7232), CachedRequestData(req_id='8', resumed_from_preemption=false, new_token_ids=[374], new_block_ids=[[]], num_computed_tokens=4530), CachedRequestData(req_id='9', resumed_from_preemption=false, new_token_ids=[22753], new_block_ids=[[]], num_computed_tokens=4530), CachedRequestData(req_id='10', resumed_from_preemption=false, new_token_ids=[64], new_block_ids=[[]], num_computed_tokens=4530), CachedRequestData(req_id='11', resumed_from_preemption=false, new_token_ids=[2176], new_block_ids=[[]], num_computed_tokens=4530), CachedRequestData(req_id='12', resumed_from_preemption=false, new_token_ids=[80], new_block_ids=[[]], num_computed_tokens=4530), CachedRequestData(req_id='13', resumed_from_preemption=false, new_token_ids=[16], new_block_ids=[[]], num_computed_tokens=4530), CachedRequestData(req_id='14', resumed_from_preemption=false, new_token_ids=[11], new_block_ids=[[]], num_computed_tokens=4530), CachedRequestData(req_id='15', resumed_from_preemption=false, new_token_ids=[323], new_block_ids=[[]], num_computed_tokens=4530), CachedRequestData(req_id='16', resumed_from_preemption=false, new_token_ids=[7947], new_block_ids=[[]], num_computed_tokens=16198), CachedRequestData(req_id='17', resumed_from_preemption=false, new_token_ids=[582], new_block_ids=[[]], num_computed_tokens=16198), CachedRequestData(req_id='18', resumed_from_preemption=false, new_token_ids=[3800], new_block_ids=[[]], num_computed_tokens=16198), CachedRequestData(req_id='19', resumed_from_preemption=false, new_token_ids=[20], new_block_ids=[[]], num_computed_tokens=16198), CachedRequestData(req_id='20', resumed_from_preemption=false, new_token_ids=[328], new_block_ids=[[]], num_computed_tokens=16198), CachedRequestData(req_id='21', resumed_from_preemption=false, new_token_ids=[4314], new_block_ids=[[]], num_computed_tokens=16198), CachedRequestData(req_id='22', resumed_from_preemption=false, new_token_ids=[1085], new_block_ids=[[]], num_computed_tokens=16198), CachedRequestData(req_id='23', resumed_from_preemption=false, new_token_ids=[701], new_block_ids=[[]], num_computed_tokens=16198), CachedRequestData(req_id='24', resumed_from_preemption=false, new_token_ids=[70581], new_block_ids=[[]], num_computed_tokens=10318), CachedRequestData(req_id='25', resumed_from_preemption=false, new_token_ids=[1895], new_block_ids=[[]], num_computed_tokens=10318), CachedRequestData(req_id='26', resumed_from_preemption=false, new_token_ids=[33356], new_block_ids=[[]], num_computed_tokens=10318), CachedRequestData(req_id='27', resumed_from_preemption=false, new_token_ids=[911], new_block_ids=[[]], num_computed_tokens=10318), CachedRequestData(req_id='28', resumed_from_preemption=false, new_token_ids=[11469], new_block_ids=[[]], num_computed_tokens=10318), CachedRequestData(req_id='29', resumed_from_preemption=false, new_token_ids=[16], new_block_ids=[[]], num_computed_tokens=10318), CachedRequestData(req_id='30', resumed_from_preemption=false, new_token_ids=[576], new_block_ids=[[]], num_computed_tokens=10318), CachedRequestData(req_id='31', resumed_from_preemption=false, new_token_ids=[13], new_block_ids=[[]], num_computed_tokens=10318), CachedRequestData(req_id='32', resumed_from_preemption=false, new_token_ids=[5944], new_block_ids=[[]], num_computed_tokens=21098), CachedRequestData(req_id='33', resumed_from_preemption=false, new_token_ids=[82], new_block_ids=[[]], num_computed_tokens=21098), CachedRequestData(req_id='34', resumed_from_preemption=false, new_token_ids=[1895], new_block_ids=[[]], num_computed_tokens=21098), CachedRequestData(req_id='35', resumed_from_preemption=false, new_token_ids=[701], new_block_ids=[[]], num_computed_tokens=21098), CachedRequestData(req_id='36', resumed_from_preemption=false, new_token_ids=[12], new_block_ids=[[]], num_computed_tokens=21098), CachedRequestData(req_id='37', resumed_from_preemption=false, new_token_ids=[92959], new_block_ids=[[]], num_computed_tokens=21098), CachedRequestData(req_id='38', resumed_from_preemption=false, new_token_ids=[13], new_block_ids=[[]], num_computed_tokens=21098), CachedRequestData(req_id='39', resumed_from_preemption=false, new_token_ids=[16128], new_block_ids=[[]], num_computed_tokens=21098), CachedRequestData(req_id='40', resumed_from_preemption=false, new_token_ids=[14649], new_block_ids=[[]], num_computed_tokens=6738), CachedRequestData(req_id='41', resumed_from_preemption=false, new_token_ids=[220], new_block_ids=[[]], num_computed_tokens=6738), CachedRequestData(req_id='42', resumed_from_preemption=false, new_token_ids=[481], new_block_ids=[[]], num_computed_tokens=6738), CachedRequestData(req_id='43', resumed_from_preemption=false, new_token_ids=[387], new_block_ids=[[]], num_computed_tokens=6738), CachedRequestData(req_id='44', resumed_from_preemption=false, new_token_ids=[264], new_block_ids=[[]], num_computed_tokens=6738), CachedRequestData(req_id='45', resumed_from_preemption=false, new_token_ids=[936], new_block_ids=[[]], num_computed_tokens=6738), CachedRequestData(req_id='46', resumed_from_preemption=false, new_token_ids=[1895], new_block_ids=[[]], num_computed_tokens=6738), CachedRequestData(req_id='47', resumed_from_preemption=false, new_token_ids=[701], new_block_ids=[[]], num_computed_tokens=6738), CachedRequestData(req_id='48', resumed_from_preemption=false, new_token_ids=[481], new_block_ids=[[]], num_computed_tokens=8406), CachedRequestData(req_id='49', resumed_from_preemption=false, new_token_ids=[264], new_block_ids=[[]], num_computed_tokens=8406), CachedRequestData(req_id='50', resumed_from_preemption=false, new_token_ids=[7892], new_block_ids=[[]], num_computed_tokens=8406), CachedRequestData(req_id='51', resumed_from_preemption=false, new_token_ids=[40540], new_block_ids=[[]], num_computed_tokens=8406), CachedRequestData(req_id='52', resumed_from_preemption=false, new_token_ids=[1660], new_block_ids=[[]], num_computed_tokens=8406), CachedRequestData(req_id='53', resumed_from_preemption=false, new_token_ids=[3359], new_block_ids=[[]], num_computed_tokens=8406), CachedRequestData(req_id='54', resumed_from_preemption=false, new_token_ids=[911], new_block_ids=[[]], num_computed_tokens=8406), CachedRequestData(req_id='55', resumed_from_preemption=false, new_token_ids=[2578], new_block_ids=[[]], num_computed_tokens=8406), CachedRequestData(req_id='56', resumed_from_preemption=false, new_token_ids=[4772], new_block_ids=[[]], num_computed_tokens=6175), CachedRequestData(req_id='57', resumed_from_preemption=false, new_token_ids=[82], new_block_ids=[[]], num_computed_tokens=6175), CachedRequestData(req_id='58', resumed_from_preemption=false, new_token_ids=[328], new_block_ids=[[]], num_computed_tokens=6175), CachedRequestData(req_id='59', resumed_from_preemption=false, new_token_ids=[364], new_block_ids=[[]], num_computed_tokens=6175), CachedRequestData(req_id='60', resumed_from_preemption=false, new_token_ids=[12], new_block_ids=[[]], num_computed_tokens=6175), CachedRequestData(req_id='61', resumed_from_preemption=false, new_token_ids=[4063], new_block_ids=[[]], num_computed_tokens=6175), CachedRequestData(req_id='62', resumed_from_preemption=false, new_token_ids=[3359], new_block_ids=[[]], num_computed_tokens=6175), CachedRequestData(req_id='63', resumed_from_preemption=false, new_token_ids=[82], new_block_ids=[[]], num_computed_tokens=6175), CachedRequestData(req_id='64', resumed_from_preemption=false, new_token_ids=[19], new_block_ids=[[8825]], num_computed_tokens=5520), CachedRequestData(req_id='65', resumed_from_preemption=false, new_token_ids=[21], new_block_ids=[[8824]], num_computed_tokens=5520), CachedRequestData(req_id='66', resumed_from_preemption=false, new_token_ids=[11], new_block_ids=[[8823]], num_computed_tokens=5520), CachedRequestData(req_id='67', resumed_from_preemption=false, new_token_ids=[7947], new_block_ids=[[8822]], num_computed_tokens=5520), CachedRequestData(req_id='68', resumed_from_preemption=false, new_token_ids=[19], new_block_ids=[[8821]], num_computed_tokens=5520), CachedRequestData(req_id='69', resumed_from_preemption=false, new_token_ids=[2578], new_block_ids=[[8820]], num_computed_tokens=5520), CachedRequestData(req_id='70', resumed_from_preemption=false, new_token_ids=[13], new_block_ids=[[8819]], num_computed_tokens=5520), CachedRequestData(req_id='71', resumed_from_preemption=false, new_token_ids=[11], new_block_ids=[[8818]], num_computed_tokens=5520), CachedRequestData(req_id='72', resumed_from_preemption=false, new_token_ids=[3359], new_block_ids=[[]], num_computed_tokens=5898), CachedRequestData(req_id='73', resumed_from_preemption=false, new_token_ids=[5262], new_block_ids=[[]], num_computed_tokens=5898), CachedRequestData(req_id='74', resumed_from_preemption=false, new_token_ids=[11358], new_block_ids=[[]], num_computed_tokens=5898), CachedRequestData(req_id='75', resumed_from_preemption=false, new_token_ids=[1895], new_block_ids=[[]], num_computed_tokens=5898), CachedRequestData(req_id='76', resumed_from_preemption=false, new_token_ids=[877], new_block_ids=[[]], num_computed_tokens=5898), CachedRequestData(req_id='77', resumed_from_preemption=false, new_token_ids=[20], new_block_ids=[[]], num_computed_tokens=5898), CachedRequestData(req_id='78', resumed_from_preemption=false, new_token_ids=[2515], new_block_ids=[[]], num_computed_tokens=5898), CachedRequestData(req_id='79', resumed_from_preemption=false, new_token_ids=[12], new_block_ids=[[]], num_computed_tokens=5898), CachedRequestData(req_id='80', resumed_from_preemption=false, new_token_ids=[1565], new_block_ids=[[]], num_computed_tokens=12913), CachedRequestData(req_id='81', resumed_from_preemption=false, new_token_ids=[279], new_block_ids=[[]], num_computed_tokens=12913), CachedRequestData(req_id='82', resumed_from_preemption=false, new_token_ids=[3893], new_block_ids=[[]], num_computed_tokens=12913), CachedRequestData(req_id='83', resumed_from_preemption=false, new_token_ids=[19], new_block_ids=[[]], num_computed_tokens=12913), CachedRequestData(req_id='84', resumed_from_preemption=false, new_token_ids=[320], new_block_ids=[[]], num_computed_tokens=12913), CachedRequestData(req_id='85', resumed_from_preemption=false, new_token_ids=[20], new_block_ids=[[]], num_computed_tokens=12913), CachedRequestData(req_id='86', resumed_from_preemption=false, new_token_ids=[773], new_block_ids=[[]], num_computed_tokens=12913), CachedRequestData(req_id='87', resumed_from_preemption=false, new_token_ids=[11], new_block_ids=[[]], num_computed_tokens=12913), CachedRequestData(req_id='88', resumed_from_preemption=false, new_token_ids=[1379], new_block_ids=[[]], num_computed_tokens=11566), CachedRequestData(req_id='89', resumed_from_preemption=false, new_token_ids=[1447], new_block_ids=[[]], num_computed_tokens=11566), CachedRequestData(req_id='90', resumed_from_preemption=false, new_token_ids=[22879], new_block_ids=[[]], num_computed_tokens=11566), CachedRequestData(req_id='91', resumed_from_preemption=false, new_token_ids=[1223], new_block_ids=[[]], num_computed_tokens=11566), CachedRequestData(req_id='92', resumed_from_preemption=false, new_token_ids=[387], new_block_ids=[[]], num_computed_tokens=11566), CachedRequestData(req_id='93', resumed_from_preemption=false, new_token_ids=[16293], new_block_ids=[[]], num_computed_tokens=11566), CachedRequestData(req_id='94', resumed_from_preemption=false, new_token_ids=[2659], new_block_ids=[[]], num_computed_tokens=11566), CachedRequestData(req_id='95', resumed_from_preemption=false, new_token_ids=[64547], new_block_ids=[[]], num_computed_tokens=11566), CachedRequestData(req_id='96', resumed_from_preemption=false, new_token_ids=[1246], new_block_ids=[[]], num_computed_tokens=10907), CachedRequestData(req_id='97', resumed_from_preemption=false, new_token_ids=[3897], new_block_ids=[[]], num_computed_tokens=10907), CachedRequestData(req_id='98', resumed_from_preemption=false, new_token_ids=[328], new_block_ids=[[]], num_computed_tokens=10907), CachedRequestData(req_id='99', resumed_from_preemption=false, new_token_ids=[20], new_block_ids=[[]], num_computed_tokens=10907), CachedRequestData(req_id='100', resumed_from_preemption=false, new_token_ids=[13394], new_block_ids=[[]], num_computed_tokens=10907), CachedRequestData(req_id='101', resumed_from_preemption=false, new_token_ids=[5548], new_block_ids=[[]], num_computed_tokens=10907), CachedRequestData(req_id='102', resumed_from_preemption=false, new_token_ids=[3565], new_block_ids=[[]], num_computed_tokens=10907), CachedRequestData(req_id='103', resumed_from_preemption=false, new_token_ids=[1895], new_block_ids=[[]], num_computed_tokens=10907), CachedRequestData(req_id='104', resumed_from_preemption=false, new_token_ids=[264], new_block_ids=[[]], num_computed_tokens=8602), CachedRequestData(req_id='105', resumed_from_preemption=false, new_token_ids=[12], new_block_ids=[[]], num_computed_tokens=8602), CachedRequestData(req_id='106', resumed_from_preemption=false, new_token_ids=[2777], new_block_ids=[[]], num_computed_tokens=8602), CachedRequestData(req_id='107', resumed_from_preemption=false, new_token_ids=[943], new_block_ids=[[]], num_computed_tokens=8602), CachedRequestData(req_id='108', resumed_from_preemption=false, new_token_ids=[576], new_block_ids=[[]], num_computed_tokens=8602), CachedRequestData(req_id='109', resumed_from_preemption=false, new_token_ids=[3830], new_block_ids=[[]], num_computed_tokens=8602), CachedRequestData(req_id='110', resumed_from_preemption=false, new_token_ids=[1895], new_block_ids=[[]], num_computed_tokens=8602), CachedRequestData(req_id='111', resumed_from_preemption=false, new_token_ids=[525], new_block_ids=[[]], num_computed_tokens=8602), CachedRequestData(req_id='112', resumed_from_preemption=false, new_token_ids=[279], new_block_ids=[[]], num_computed_tokens=13530), CachedRequestData(req_id='113', resumed_from_preemption=false, new_token_ids=[3070], new_block_ids=[[]], num_computed_tokens=13530), CachedRequestData(req_id='114', resumed_from_preemption=false, new_token_ids=[17112], new_block_ids=[[]], num_computed_tokens=13530), CachedRequestData(req_id='115', resumed_from_preemption=false, new_token_ids=[1752], new_block_ids=[[]], num_computed_tokens=13530), CachedRequestData(req_id='116', resumed_from_preemption=false, new_token_ids=[877], new_block_ids=[[]], num_computed_tokens=13530), CachedRequestData(req_id='117', resumed_from_preemption=false, new_token_ids=[11], new_block_ids=[[]], num_computed_tokens=13530), CachedRequestData(req_id='118', resumed_from_preemption=false, new_token_ids=[279], new_block_ids=[[]], num_computed_tokens=13530), CachedRequestData(req_id='119', resumed_from_preemption=false, new_token_ids=[7947], new_block_ids=[[]], num_computed_tokens=13530)], num_scheduled_tokens={64: 1, 60: 1, 111: 1, 12: 1, 113: 1, 23: 1, 80: 1, 87: 1, 37: 1, 63: 1, 84: 1, 36: 1, 47: 1, 19: 1, 3: 1, 16: 1, 100: 1, 55: 1, 25: 1, 51: 1, 93: 1, 70: 1, 57: 1, 90: 1, 101: 1, 34: 1, 98: 1, 92: 1, 106: 1, 31: 1, 8: 1, 107: 1, 108: 1, 38: 1, 15: 1, 99: 1, 0: 1, 56: 1, 58: 1, 14: 1, 69: 1, 110: 1, 95: 1, 24: 1, 75: 1, 115: 1, 66: 1, 52: 1, 1: 1, 18: 1, 28: 1, 82: 1, 61: 1, 85: 1, 21: 1, 109: 1, 33: 1, 59: 1, 35: 1, 32: 1, 119: 1, 94: 1, 97: 1, 68: 1, 10: 1, 89: 1, 71: 1, 65: 1, 40: 1, 17: 1, 88: 1, 91: 1, 96: 1, 20: 1, 73: 1, 49: 1, 53: 1, 86: 1, 2: 1, 112: 1, 81: 1, 54: 1, 77: 1, 117: 1, 43: 1, 103: 1, 46: 1, 27: 1, 67: 1, 9: 1, 26: 1, 4: 1, 48: 1, 118: 1, 6: 1, 50: 1, 29: 1, 11: 1, 116: 1, 30: 1, 79: 1, 62: 1, 78: 1, 83: 1, 104: 1, 13: 1, 22: 1, 41: 1, 105: 1, 76: 1, 72: 1, 44: 1, 39: 1, 102: 1, 5: 1, 42: 1, 74: 1, 114: 1, 7: 1, 45: 1}, total_num_scheduled_tokens=120, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[36], finished_req_ids=[], free_encoder_input_ids=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unsloth_train\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m trainer_stats = \u001b[43munsloth_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/unsloth/trainer.py:45\u001b[39m, in \u001b[36munsloth_train\u001b[39m\u001b[34m(trainer, *args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munsloth_train\u001b[39m(trainer, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/transformers/trainer.py:2207\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2205\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:320\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:28\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/trl/extras/profiling.py:98\u001b[39m, in \u001b[36mprofiling_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, func.\u001b[34m__name__\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/unsloth_compiled_cache/UnslothGRPOTrainer.py:1604\u001b[39m, in \u001b[36m_UnslothGRPOTrainer._prepare_inputs\u001b[39m\u001b[34m(self, generation_batch)\u001b[39m\n\u001b[32m   1601\u001b[39m generate_every = \u001b[38;5;28mself\u001b[39m.args.steps_per_generation * \u001b[38;5;28mself\u001b[39m.num_iterations\n\u001b[32m   1602\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._step % generate_every == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffered_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1603\u001b[39m     \u001b[38;5;66;03m# self._buffered_inputs=None can occur when resuming from a checkpoint\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1604\u001b[39m     generation_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_and_score_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1605\u001b[39m     generation_batch = shuffle_tensor_dict(generation_batch)\n\u001b[32m   1606\u001b[39m     \u001b[38;5;28mself\u001b[39m._buffered_inputs = split_tensor_dict(generation_batch, \u001b[38;5;28mself\u001b[39m.args.steps_per_generation)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/unsloth_compiled_cache/UnslothGRPOTrainer.py:1759\u001b[39m, in \u001b[36m_UnslothGRPOTrainer._generate_and_score_completions\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m   1756\u001b[39m     all_prompts_text = prompts_text\n\u001b[32m   1758\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvLLM.generate\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1759\u001b[39m     all_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_prompts_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgrpo_trainer_lora_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1761\u001b[39m completion_ids = [output.token_ids \u001b[38;5;28;01mfor\u001b[39;00m outputs \u001b[38;5;129;01min\u001b[39;00m all_outputs \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs.outputs]\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vllm_tensor_parallel_size > \u001b[32m1\u001b[39m:\n\u001b[32m   1764\u001b[39m     \u001b[38;5;66;03m# Slice completions for this rank within its TP group.\u001b[39;00m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;66;03m# Each rank generates all outputs â€” we keep only our share.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/vllm/utils.py:1267\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1260\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1262\u001b[39m         warnings.warn(\n\u001b[32m   1263\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1264\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1265\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:474\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[39m\n\u001b[32m    462\u001b[39m     sampling_params = \u001b[38;5;28mself\u001b[39m.get_default_sampling_params()\n\u001b[32m    464\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    465\u001b[39m     prompts=parsed_prompts,\n\u001b[32m    466\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    471\u001b[39m     priority=priority,\n\u001b[32m    472\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:1517\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m   1515\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m   1516\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m-> \u001b[39m\u001b[32m1517\u001b[39m     step_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1518\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m   1519\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:232\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[32m    235\u001b[39m iteration_stats = IterationStats() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:226\u001b[39m, in \u001b[36mInprocClient.get_output\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_output\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> EngineCoreOutputs:\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     outputs, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs.get(\u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m EngineCoreOutputs()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/vllm/v1/engine/core.py:231\u001b[39m, in \u001b[36mEngineCore.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    230\u001b[39m scheduler_output = \u001b[38;5;28mself\u001b[39m.scheduler.schedule()\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m model_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheduler_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m engine_core_outputs = \u001b[38;5;28mself\u001b[39m.scheduler.update_from_output(\n\u001b[32m    233\u001b[39m     scheduler_output, model_output)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (engine_core_outputs,\n\u001b[32m    236\u001b[39m         scheduler_output.total_num_scheduled_tokens > \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/vllm/v1/engine/core.py:217\u001b[39m, in \u001b[36mEngineCore.execute_model\u001b[39m\u001b[34m(self, scheduler_output)\u001b[39m\n\u001b[32m    214\u001b[39m dump_engine_exception(\u001b[38;5;28mself\u001b[39m.vllm_config, scheduler_output,\n\u001b[32m    215\u001b[39m                       \u001b[38;5;28mself\u001b[39m.scheduler.make_stats())\n\u001b[32m    216\u001b[39m \u001b[38;5;66;03m# Re-raise exception\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/vllm/v1/engine/core.py:211\u001b[39m, in \u001b[36mEngineCore.execute_model\u001b[39m\u001b[34m(self, scheduler_output)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, scheduler_output: SchedulerOutput):\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheduler_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    213\u001b[39m         \u001b[38;5;66;03m# NOTE: This method is exception-free\u001b[39;00m\n\u001b[32m    214\u001b[39m         dump_engine_exception(\u001b[38;5;28mself\u001b[39m.vllm_config, scheduler_output,\n\u001b[32m    215\u001b[39m                               \u001b[38;5;28mself\u001b[39m.scheduler.make_stats())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py:87\u001b[39m, in \u001b[36mExecutor.execute_model\u001b[39m\u001b[34m(self, scheduler_output)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_model\u001b[39m(\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     85\u001b[39m     scheduler_output,\n\u001b[32m     86\u001b[39m ) -> Union[ModelRunnerOutput, Future[ModelRunnerOutput]]:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexecute_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheduler_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py:57\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/vllm/utils.py:2671\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   2669\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2670\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2671\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py:293\u001b[39m, in \u001b[36mWorker.execute_model\u001b[39m\u001b[34m(self, scheduler_output)\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group().is_first_rank:\n\u001b[32m    289\u001b[39m     intermediate_tensors = IntermediateTensors(\n\u001b[32m    290\u001b[39m         get_pp_group().recv_tensor_dict(\n\u001b[32m    291\u001b[39m             all_gather_group=get_tp_group()))\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheduler_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m parallel_config = \u001b[38;5;28mself\u001b[39m.vllm_config.parallel_config\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parallel_config.distributed_executor_backend != \u001b[33m\"\u001b[39m\u001b[33mexternal_launcher\u001b[39m\u001b[33m\"\u001b[39m \\\n\u001b[32m    297\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group().is_last_rank:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cti-model-training/venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py:1374\u001b[39m, in \u001b[36mGPUModelRunner.execute_model\u001b[39m\u001b[34m(self, scheduler_output, intermediate_tensors)\u001b[39m\n\u001b[32m   1371\u001b[39m max_gen_len = sampled_token_ids.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_gen_len == \u001b[32m1\u001b[39m:\n\u001b[32m   1373\u001b[39m     \u001b[38;5;66;03m# No spec decode tokens.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m     valid_sampled_token_ids = \u001b[43msampled_token_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1376\u001b[39m     \u001b[38;5;66;03m# Includes spec decode tokens.\u001b[39;00m\n\u001b[32m   1377\u001b[39m     valid_sampled_token_ids = \u001b[38;5;28mself\u001b[39m.rejection_sampler.parse_output(\n\u001b[32m   1378\u001b[39m         sampled_token_ids,\n\u001b[32m   1379\u001b[39m         \u001b[38;5;28mself\u001b[39m.input_batch.vocab_size,\n\u001b[32m   1380\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "# Start training\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891aa65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
