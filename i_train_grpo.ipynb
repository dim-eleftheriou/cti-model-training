{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "num_proc = cpu_count()\n",
    "\n",
    "import yaml\n",
    "\n",
    "from dataprep.stix.StixConfig import StixToPydanticMap, STIX, CustomSTIX\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "\n",
    "from evaluation.stix_evaluator import STIXEvaluator\n",
    "\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import torch\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "from data_processor import SplittedJsonIoDataset\n",
    "from customs import customize_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee74d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e89ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"grpo_config.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"/mnt/data/training-outputs/Llama/Llama-3.1-8B-Instruct-Not-Quantized/checkpoint-190\",\n",
    "#     fast_inference = True,\n",
    "#     load_in_4bit = False,\n",
    "#     max_seq_length = None,\n",
    "#     gpu_memory_utilization = 0.7\n",
    "# )\n",
    "# model = model.merge_and_unload()\n",
    "# model.save_pretrained(\"grpo_model_input\")\n",
    "# tokenizer.save_pretrained(\"grpo_model_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac42faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if model is a peft model\n",
    "# import peft.helpers\n",
    "\n",
    "# def load_model_and_tokenizer(model_name_or_path, config, **kwargs):\n",
    "#     if peft.helpers.check_if_peft_model(model_name_or_path):\n",
    "#         model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#                 model_name=model_name_or_path,\n",
    "#                 **kwargs\n",
    "#             )\n",
    "#         if config[\"merge_peft_model\"]:\n",
    "#             model.merge_and_unload()\n",
    "#     else:\n",
    "#         model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#             **config[\"model_loading_args\"]\n",
    "#         )\n",
    "#     return model, tokenizer\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"grpo_model_input\",\n",
    "    fast_inference = True,\n",
    "    load_in_4bit = True,\n",
    "    max_seq_length = None,\n",
    "    gpu_memory_utilization = 0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38459cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.merge_and_unload()\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b869905",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 32,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, tokenizer = customize_tokenizer(model, tokenizer, config)\n",
    "# no need for deepseek\n",
    "# config[\"chat_template\"] = \"deepseek\"\n",
    "# tokenizer = get_chat_template(tokenizer, config[\"chat_template\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c688ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for training\n",
    "dataset = SplittedJsonIoDataset(tokenizer, config).grpo_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "# Concatenate eval and train dataset to increase the learning examples\n",
    "train_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"eval\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a82611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_answer(answer: str) -> dict:\n",
    "    return json.loads(answer)\n",
    "\n",
    "def deserialize_response_for_evaluation(answer: str) -> dict:\n",
    "    if is_stix_bundle(answer):\n",
    "        return json.loads(answer)\n",
    "    else:\n",
    "        return {\"id\":\"\", \"type\":\"bundle\", \"objects\":[]}\n",
    "\n",
    "def extract_xml_answer(response: str) -> str:\n",
    "    answer = response.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def is_stix_bundle(text: str) -> bool:\n",
    "    try:\n",
    "        bundle = json.loads(text)\n",
    "        pydantic_stix_bundle = STIX(**bundle)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def is_custom_stix_bundle(text: str) -> bool:\n",
    "    try:\n",
    "        bundle = json.loads(text)\n",
    "        pydantic_stix_bundle = CustomSTIX(**bundle)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def count_valid_stix_objects(text: str) -> bool:\n",
    "    smap = StixToPydanticMap()\n",
    "    cnt = 0.0\n",
    "    if is_stix_bundle(text):\n",
    "        bundle = json.loads(text)\n",
    "        for obj in bundle[\"objects\"]:\n",
    "            try:\n",
    "                smap(obj)\n",
    "                cnt += 1\n",
    "            except:\n",
    "            #except ValidationError:\n",
    "                pass\n",
    "        return cnt / len(bundle[\"objects\"])\n",
    "    else:\n",
    "      return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39fdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<think>.*?</think>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def stix_validity_reward_func(completions, answers, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion can is a stix bundle.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if is_stix_bundle(r) else 0.0 for r in extracted_responses]\n",
    "\n",
    "def custom_stix_validity_reward_func(completions, answers, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion can is a stix bundle.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if is_custom_stix_bundle(r) else 0.0 for r in extracted_responses]\n",
    "\n",
    "def stix_objects_validity_reward_func(completions, answers, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has valid stix objects.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 * count_valid_stix_objects(r) for r in extracted_responses]\n",
    "\n",
    "def accuracy_reward_func(completions, answers, **kwargs) -> list[float]:\n",
    "    evaluator = STIXEvaluator()\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    desirialized_responses = [deserialize_response_for_evaluation(r) for r in extracted_responses]\n",
    "    desirialized_answers = [deserialize_answer(a) for a in answers]\n",
    "    return [evaluator.evaluate_single(r, a)[2] for r, a in zip(desirialized_responses, desirialized_answers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference!\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4, # Increase to 4 for smoother training\n",
    "    num_generations = 2, # Decrease if out of memory\n",
    "    max_prompt_length = 8000, #config[\"model_loading_args\"][\"max_seq_length\"],\n",
    "    max_completion_length = 8000, #config[\"model_loading_args\"][\"max_seq_length\"],\n",
    "    num_train_epochs = 2, # Set to 1 for a full training run\n",
    "    save_steps = 250,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"tensorboard\", # Can use Weights & Biases\n",
    "    output_dir = \"grpo_outputs\",\n",
    "    ###############################\n",
    "    temperature=0.7,\n",
    "    top_p=0.6,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00505a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        format_reward_func,\n",
    "        stix_validity_reward_func,\n",
    "        custom_stix_validity_reward_func,\n",
    "        stix_objects_validity_reward_func,\n",
    "        accuracy_reward_func\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4747f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3142d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import unsloth_train\n",
    "# Start training\n",
    "trainer.train()\n",
    "#trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891aa65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
